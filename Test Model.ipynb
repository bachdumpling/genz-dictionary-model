{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a063150c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tutorial: https://www.analyticssteps.com/blogs/nltk-python-tutorial-beginners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c1197fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/phuongle/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/phuongle/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#load library\n",
    "import requests\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fec04f69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I swear I thought the stork actually rang the bell until o saw the baby and got the joke😂😂😂 cause after what I’ve seen animals can do now phewww😂', 'am I the only one wondering how she did this?!', 'You gotta number for that stork, cause i want one.', 'Omg how cute is this!!! When she/he asks where I came from you have proof', 'so cute🥰', 'This is amazing', 'You can order them on Amazon and they send it to your door along with the baby', 'My stork was lost for many years. Thankfully they found their way to my home 💕. This is just so precious 🥺', 'I KNEW THATS HOW IT HAPPENS!!!!', 'That’s soooo cute!', 'This is so cute!!!', 'The lighting, sounds 🥺 this is pure perfection!!', 'That is one of the best things I have seen on this app!! Congratulations 🎉💕', 'So creative! love it!!! Congratulations!', 'Omg HOW COOL', '😂😂😂😂do they accept returns', 'This is so adorable. Love this💗💗', 'That is toooooo cute💜', 'congrats for a New born', '🥰🥰', 'That dog said to hell with your new carpet 😂', '“That’s a new carpet” the dog “new you say 😏“', 'the Chihuahua stress level with those kittens pulling up', 'the first one 💀', 'The 2 cats when the dog ran😂😂🤣', 'all the dogs understood all the assignments', 'the dog and the carpet omg 😂', 'bruh not the new carpeet💀💀💀', 'The last one!', 'he sed:fluff your new carpet', 'That chihuahua was scared of them kittens😂', 'long lives THE KING', '@orksococosoe THE NEW CARPET MADE ME GIGGLE', '@mikufan_39 omg', 'The \"Mario Dies\" theme at the end 🤣🤣🤣', '🥰😂', '😂😂😂😂😂', 'THE KITTENS ARE SO CUTEEEEE', '@thehybridyt 💀', '😂🥰😂🥰', 'so cute 😂😂😂', 'so sweet', 'it\\'s the middle goat\\'s \"meeee\"😅😅😅', 'Omg so cute!', 'The 2nd one cracked me up 😂😂😂😂😂😂😂😂😂😂😂', 'I need to start a tiny goat family of my own🥺', 'YOU ALL HAVE A WHOLE SYMPHONY. LOVE THE SOLO BABY AT THE END..', 'First one all I could hear is Toy Story... \"The CLAWWWWW\" 😂', 'Whew! I feel better now 😊', 'Why, of the hundreds of tiktok I saw tonight, this one make me to laugh to tears? 😂', 'Nowadays animals really clever 😂😂😂', 'Choir present best song -nothing better then singing with your bodies😂', 'Just too adorable 🥰 🥰', 'Omg so cute', 'That’s how I’m feeling too…mehhh!', \"😂😂😂make my day watching this oh my G don't kill them\", 'In America, its Baaaa! 😳', \"They're so cuteeee 😭\", \"That's it I need a farm\", 'I’m gonna watch this every day, just because it’s the best thing I’ve ever seen and it makes me smile🥰', 'I miss when my cats were kittens', 'I wish cats could stay kittens forever', '4 years and 20 pounds later, mine still has no person space', 'I miss that. the softness, the weight. the paw on my face and listening breathe. now. they do curl next to me at night. Cats are the best.', 'The love of a cat makes life worthwhile.', 'Omg I cant take the cuteness overload 😍😍U0001faf6❤️❤️', 'Yeah they’re like that when they were little mine is not doing this in a year I miss that🥺', 'he’s so real for this', 'my kitty does the same ...she has to be close to me at all times...i am her personal property', 'When my cat was a kitten she would always sleep on my neck and this one night I woke up to her sprawled all over my neck😭', 'I miss when my cat was a kitten now she doesn’t even want to sit near me 😭', 'i need a cat', 'the tail mustache 🥰', 'This is why i want a cat', 'miss when my cats were babies and would actually cuddle w me :(', 'i miss this..', 'I WANT IT', 'i miss when my cat did this man', 'This is so cute', 'That’s a baby cat', '😀', '🤣😂🤣😂', '😂😂😂😂😂', '😂😂😂', '😂😂😂', '🥰😂', '😂😂😂', '😅😂😂😂', '😂😂😂😂', '😂😂😂', '😂😂😂', '😁', '😂😂', '😂', '😂😂😂😂', '😂😂', '😂😂😂😂', '😂', '😂😂😂', 'pov you older siblings asking for a sip', \"ok ok it's all urs\", 'Polite burp', 'I can’t I’d hiccup straight away 😂😂', 'How can she do that', '\"can I have a sip?\"', \"don't really get it?\", '@ㄒ尺丨乂 me when I say can i have a sip of your drink', 'I would be crying U0001f979', '@explodingburgerzz me when im home alone', 'Me with Diet Coke', '@caseyteaxx me with cola 😂', 'Defo hungover', '@JU0001faac you', 'me in the gym on the treadmill 😂', 'fr me with Radnor fizz', '@U0001fae7 so you', 'I drank a bottle like this once and I threw up loads after', \"@heisenberg_gn and it's cold...I feel pain\", '@tessie♡ the burps are so couqette', 'I refuse to believe she uses even half of that', 'sephora dupe', 'so satisfying but so much waste😢', 'half of these products are never going to get touched again', 'it just kept going', 'i think she likes makeup guys', \"It's giving drumline :)\", 'and my mom tells me i own too much makeup💀', 'Me in 5 years', 'if only I had the money 💔', 'I don’t have that much', 'ILL TAKE a COUPLE 🐥😭', 'How is this even sustainable? Environmentally & use wise😪', 'same', 'Shawty buy every product of each brand', 'How many of those you think are expired?', 'someone tell me why i watched this whole thing', 'i aspire to be like this', 'Imagine how expensive all this is', 'That’s my favorite hole life savings lol', 'Best story ever! I need a part 2 from baby!', 'i love baby babble so much', 'baby fever, Baby Fever, BABY FEVER!!!', 'the wrist roll 😭😭😭 so cute', 'babies listening to their own voices is just so cute!', \"It's funny to think about how babies probably don't know when they say their first word because they probably think they're talking all the time.\", 'Omg her covering her mouth while yawning… ugh adorable ☺️', 'That was the most in depth story ever! I was hooked', \"that's how her day went😂😂\", 'the covering her mouth for the yawn 🥺❤️', 'i love baby voices 🥺🥺', 'oh my goodness she is so precious U0001f979', 'The caption U0001f979U0001f979', '🥱🥱🥱oh my goodness🥰🥰🥰', \"it's like she's telling a very serious story about dada!😂😂🥰🥰🥰\", 'I don’t want another baby i don’t want another baby i don’t want another baby 😂😂😩😩😩😩', 'her covering her yawn 🥺', 'OMG . So cute 🥰🥰', 'She is so adorable!!', 'Oh no she covered her mouth yawning!!!! That is so cute']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load json data from github \n",
    "# import urllib library\n",
    "from urllib.request import urlopen\n",
    "  \n",
    "# import json\n",
    "import json\n",
    "# store the URL in url as \n",
    "# parameter for urlopen\n",
    "url = \"https://raw.githubusercontent.com/bachdumpling/genz-dictionary-model/main/comments.json\"\n",
    "  \n",
    "# store the response of URL\n",
    "response = urlopen(url)\n",
    "  \n",
    "# storing the JSON response \n",
    "# from url in data\n",
    "dataset = json.loads(response.read())\n",
    "  \n",
    "# print the json response\n",
    "print(dataset)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "274686c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'swear', 'I', 'thought', 'the', 'stork', 'actually', 'rang', 'the', 'bell', 'until', 'o', 'saw', 'the', 'baby', 'and', 'got', 'the', 'joke😂😂😂', 'cause', 'after', 'what', 'I', '’', 've', 'seen', 'animals', 'can', 'do', 'now', 'phewww😂'], ['am', 'I', 'the', 'only', 'one', 'wondering', 'how', 'she', 'did', 'this', '?', '!'], ['You', 'got', 'ta', 'number', 'for', 'that', 'stork', ',', 'cause', 'i', 'want', 'one', '.'], ['Omg', 'how', 'cute', 'is', 'this', '!', '!', '!', 'When', 'she/he', 'asks', 'where', 'I', 'came', 'from', 'you', 'have', 'proof'], ['so', 'cute🥰'], ['This', 'is', 'amazing'], ['You', 'can', 'order', 'them', 'on', 'Amazon', 'and', 'they', 'send', 'it', 'to', 'your', 'door', 'along', 'with', 'the', 'baby'], ['My', 'stork', 'was', 'lost', 'for', 'many', 'years', '.', 'Thankfully', 'they', 'found', 'their', 'way', 'to', 'my', 'home', '💕', '.', 'This', 'is', 'just', 'so', 'precious', '🥺'], ['I', 'KNEW', 'THATS', 'HOW', 'IT', 'HAPPENS', '!', '!', '!', '!'], ['That', '’', 's', 'soooo', 'cute', '!'], ['This', 'is', 'so', 'cute', '!', '!', '!'], ['The', 'lighting', ',', 'sounds', '🥺', 'this', 'is', 'pure', 'perfection', '!', '!'], ['That', 'is', 'one', 'of', 'the', 'best', 'things', 'I', 'have', 'seen', 'on', 'this', 'app', '!', '!', 'Congratulations', '🎉💕'], ['So', 'creative', '!', 'love', 'it', '!', '!', '!', 'Congratulations', '!'], ['Omg', 'HOW', 'COOL'], ['😂😂😂😂do', 'they', 'accept', 'returns'], ['This', 'is', 'so', 'adorable', '.', 'Love', 'this💗💗'], ['That', 'is', 'toooooo', 'cute💜'], ['congrats', 'for', 'a', 'New', 'born'], ['🥰🥰'], ['That', 'dog', 'said', 'to', 'hell', 'with', 'your', 'new', 'carpet', '😂'], ['“', 'That', '’', 's', 'a', 'new', 'carpet', '”', 'the', 'dog', '“', 'new', 'you', 'say', '😏', '“'], ['the', 'Chihuahua', 'stress', 'level', 'with', 'those', 'kittens', 'pulling', 'up'], ['the', 'first', 'one', '💀'], ['The', '2', 'cats', 'when', 'the', 'dog', 'ran😂😂🤣'], ['all', 'the', 'dogs', 'understood', 'all', 'the', 'assignments'], ['the', 'dog', 'and', 'the', 'carpet', 'omg', '😂'], ['bruh', 'not', 'the', 'new', 'carpeet💀💀💀'], ['The', 'last', 'one', '!'], ['he', 'sed', ':', 'fluff', 'your', 'new', 'carpet'], ['That', 'chihuahua', 'was', 'scared', 'of', 'them', 'kittens😂'], ['long', 'lives', 'THE', 'KING'], ['@', 'orksococosoe', 'THE', 'NEW', 'CARPET', 'MADE', 'ME', 'GIGGLE'], ['@', 'mikufan_39', 'omg'], ['The', '``', 'Mario', 'Dies', \"''\", 'theme', 'at', 'the', 'end', '🤣🤣🤣'], ['🥰😂'], ['😂😂😂😂😂'], ['THE', 'KITTENS', 'ARE', 'SO', 'CUTEEEEE'], ['@', 'thehybridyt', '💀'], ['😂🥰😂🥰'], ['so', 'cute', '😂😂😂'], ['so', 'sweet'], ['it', \"'s\", 'the', 'middle', 'goat', \"'s\", '``', 'meeee', \"''\", '😅😅😅'], ['Omg', 'so', 'cute', '!'], ['The', '2nd', 'one', 'cracked', 'me', 'up', '😂😂😂😂😂😂😂😂😂😂😂'], ['I', 'need', 'to', 'start', 'a', 'tiny', 'goat', 'family', 'of', 'my', 'own🥺'], ['YOU', 'ALL', 'HAVE', 'A', 'WHOLE', 'SYMPHONY', '.', 'LOVE', 'THE', 'SOLO', 'BABY', 'AT', 'THE', 'END', '..'], ['First', 'one', 'all', 'I', 'could', 'hear', 'is', 'Toy', 'Story', '...', '``', 'The', 'CLAWWWWW', \"''\", '😂'], ['Whew', '!', 'I', 'feel', 'better', 'now', '😊'], ['Why', ',', 'of', 'the', 'hundreds', 'of', 'tiktok', 'I', 'saw', 'tonight', ',', 'this', 'one', 'make', 'me', 'to', 'laugh', 'to', 'tears', '?', '😂'], ['Nowadays', 'animals', 'really', 'clever', '😂😂😂'], ['Choir', 'present', 'best', 'song', '-nothing', 'better', 'then', 'singing', 'with', 'your', 'bodies😂'], ['Just', 'too', 'adorable', '🥰', '🥰'], ['Omg', 'so', 'cute'], ['That', '’', 's', 'how', 'I', '’', 'm', 'feeling', 'too…mehhh', '!'], ['😂😂😂make', 'my', 'day', 'watching', 'this', 'oh', 'my', 'G', 'do', \"n't\", 'kill', 'them'], ['In', 'America', ',', 'its', 'Baaaa', '!', '😳'], ['They', \"'re\", 'so', 'cuteeee', '😭'], ['That', \"'s\", 'it', 'I', 'need', 'a', 'farm'], ['I', '’', 'm', 'gon', 'na', 'watch', 'this', 'every', 'day', ',', 'just', 'because', 'it', '’', 's', 'the', 'best', 'thing', 'I', '’', 've', 'ever', 'seen', 'and', 'it', 'makes', 'me', 'smile🥰'], ['I', 'miss', 'when', 'my', 'cats', 'were', 'kittens'], ['I', 'wish', 'cats', 'could', 'stay', 'kittens', 'forever'], ['4', 'years', 'and', '20', 'pounds', 'later', ',', 'mine', 'still', 'has', 'no', 'person', 'space'], ['I', 'miss', 'that', '.', 'the', 'softness', ',', 'the', 'weight', '.', 'the', 'paw', 'on', 'my', 'face', 'and', 'listening', 'breathe', '.', 'now', '.', 'they', 'do', 'curl', 'next', 'to', 'me', 'at', 'night', '.', 'Cats', 'are', 'the', 'best', '.'], ['The', 'love', 'of', 'a', 'cat', 'makes', 'life', 'worthwhile', '.'], ['Omg', 'I', 'cant', 'take', 'the', 'cuteness', 'overload', '😍😍U0001faf6❤️❤️'], ['Yeah', 'they', '’', 're', 'like', 'that', 'when', 'they', 'were', 'little', 'mine', 'is', 'not', 'doing', 'this', 'in', 'a', 'year', 'I', 'miss', 'that🥺'], ['he', '’', 's', 'so', 'real', 'for', 'this'], ['my', 'kitty', 'does', 'the', 'same', '...', 'she', 'has', 'to', 'be', 'close', 'to', 'me', 'at', 'all', 'times', '...', 'i', 'am', 'her', 'personal', 'property'], ['When', 'my', 'cat', 'was', 'a', 'kitten', 'she', 'would', 'always', 'sleep', 'on', 'my', 'neck', 'and', 'this', 'one', 'night', 'I', 'woke', 'up', 'to', 'her', 'sprawled', 'all', 'over', 'my', 'neck😭'], ['I', 'miss', 'when', 'my', 'cat', 'was', 'a', 'kitten', 'now', 'she', 'doesn', '’', 't', 'even', 'want', 'to', 'sit', 'near', 'me', '😭'], ['i', 'need', 'a', 'cat'], ['the', 'tail', 'mustache', '🥰'], ['This', 'is', 'why', 'i', 'want', 'a', 'cat'], ['miss', 'when', 'my', 'cats', 'were', 'babies', 'and', 'would', 'actually', 'cuddle', 'w', 'me', ':', '('], ['i', 'miss', 'this', '..'], ['I', 'WANT', 'IT'], ['i', 'miss', 'when', 'my', 'cat', 'did', 'this', 'man'], ['This', 'is', 'so', 'cute'], ['That', '’', 's', 'a', 'baby', 'cat'], ['😀'], ['🤣😂🤣😂'], ['😂😂😂😂😂'], ['😂😂😂'], ['😂😂😂'], ['🥰😂'], ['😂😂😂'], ['😅😂😂😂'], ['😂😂😂😂'], ['😂😂😂'], ['😂😂😂'], ['😁'], ['😂😂'], ['😂'], ['😂😂😂😂'], ['😂😂'], ['😂😂😂😂'], ['😂'], ['😂😂😂'], ['pov', 'you', 'older', 'siblings', 'asking', 'for', 'a', 'sip'], ['ok', 'ok', 'it', \"'s\", 'all', 'urs'], ['Polite', 'burp'], ['I', 'can', '’', 't', 'I', '’', 'd', 'hiccup', 'straight', 'away', '😂😂'], ['How', 'can', 'she', 'do', 'that'], ['``', 'can', 'I', 'have', 'a', 'sip', '?', \"''\"], ['do', \"n't\", 'really', 'get', 'it', '?'], ['@', 'ㄒ尺丨乂', 'me', 'when', 'I', 'say', 'can', 'i', 'have', 'a', 'sip', 'of', 'your', 'drink'], ['I', 'would', 'be', 'crying', 'U0001f979'], ['@', 'explodingburgerzz', 'me', 'when', 'im', 'home', 'alone'], ['Me', 'with', 'Diet', 'Coke'], ['@', 'caseyteaxx', 'me', 'with', 'cola', '😂'], ['Defo', 'hungover'], ['@', 'JU0001faac', 'you'], ['me', 'in', 'the', 'gym', 'on', 'the', 'treadmill', '😂'], ['fr', 'me', 'with', 'Radnor', 'fizz'], ['@', 'U0001fae7', 'so', 'you'], ['I', 'drank', 'a', 'bottle', 'like', 'this', 'once', 'and', 'I', 'threw', 'up', 'loads', 'after'], ['@', 'heisenberg_gn', 'and', 'it', \"'s\", 'cold', '...', 'I', 'feel', 'pain'], ['@', 'tessie♡', 'the', 'burps', 'are', 'so', 'couqette'], ['I', 'refuse', 'to', 'believe', 'she', 'uses', 'even', 'half', 'of', 'that'], ['sephora', 'dupe'], ['so', 'satisfying', 'but', 'so', 'much', 'waste😢'], ['half', 'of', 'these', 'products', 'are', 'never', 'going', 'to', 'get', 'touched', 'again'], ['it', 'just', 'kept', 'going'], ['i', 'think', 'she', 'likes', 'makeup', 'guys'], ['It', \"'s\", 'giving', 'drumline', ':', ')'], ['and', 'my', 'mom', 'tells', 'me', 'i', 'own', 'too', 'much', 'makeup💀'], ['Me', 'in', '5', 'years'], ['if', 'only', 'I', 'had', 'the', 'money', '💔'], ['I', 'don', '’', 't', 'have', 'that', 'much'], ['ILL', 'TAKE', 'a', 'COUPLE', '🐥😭'], ['How', 'is', 'this', 'even', 'sustainable', '?', 'Environmentally', '&', 'use', 'wise😪'], ['same'], ['Shawty', 'buy', 'every', 'product', 'of', 'each', 'brand'], ['How', 'many', 'of', 'those', 'you', 'think', 'are', 'expired', '?'], ['someone', 'tell', 'me', 'why', 'i', 'watched', 'this', 'whole', 'thing'], ['i', 'aspire', 'to', 'be', 'like', 'this'], ['Imagine', 'how', 'expensive', 'all', 'this', 'is'], ['That', '’', 's', 'my', 'favorite', 'hole', 'life', 'savings', 'lol'], ['Best', 'story', 'ever', '!', 'I', 'need', 'a', 'part', '2', 'from', 'baby', '!'], ['i', 'love', 'baby', 'babble', 'so', 'much'], ['baby', 'fever', ',', 'Baby', 'Fever', ',', 'BABY', 'FEVER', '!', '!', '!'], ['the', 'wrist', 'roll', '😭😭😭', 'so', 'cute'], ['babies', 'listening', 'to', 'their', 'own', 'voices', 'is', 'just', 'so', 'cute', '!'], ['It', \"'s\", 'funny', 'to', 'think', 'about', 'how', 'babies', 'probably', 'do', \"n't\", 'know', 'when', 'they', 'say', 'their', 'first', 'word', 'because', 'they', 'probably', 'think', 'they', \"'re\", 'talking', 'all', 'the', 'time', '.'], ['Omg', 'her', 'covering', 'her', 'mouth', 'while', 'yawning…', 'ugh', 'adorable', '☺️'], ['That', 'was', 'the', 'most', 'in', 'depth', 'story', 'ever', '!', 'I', 'was', 'hooked'], ['that', \"'s\", 'how', 'her', 'day', 'went😂😂'], ['the', 'covering', 'her', 'mouth', 'for', 'the', 'yawn', '🥺❤️'], ['i', 'love', 'baby', 'voices', '🥺🥺'], ['oh', 'my', 'goodness', 'she', 'is', 'so', 'precious', 'U0001f979'], ['The', 'caption', 'U0001f979U0001f979'], ['🥱🥱🥱oh', 'my', 'goodness🥰🥰🥰'], ['it', \"'s\", 'like', 'she', \"'s\", 'telling', 'a', 'very', 'serious', 'story', 'about', 'dada', '!', '😂😂🥰🥰🥰'], ['I', 'don', '’', 't', 'want', 'another', 'baby', 'i', 'don', '’', 't', 'want', 'another', 'baby', 'i', 'don', '’', 't', 'want', 'another', 'baby', '😂😂😩😩😩😩'], ['her', 'covering', 'her', 'yawn', '🥺'], ['OMG', '.', 'So', 'cute', '🥰🥰'], ['She', 'is', 'so', 'adorable', '!', '!'], ['Oh', 'no', 'she', 'covered', 'her', 'mouth', 'yawning', '!', '!', '!', '!', 'That', 'is', 'so', 'cute']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Cleaning \n",
    "accepted_sentence = []\n",
    "\n",
    "#Word Tokenization\n",
    "for sentence in dataset: \n",
    "    tokenized = word_tokenize(sentence)\n",
    "    accepted_sentence.append(tokenized)\n",
    "    \n",
    "print(accepted_sentence)\n",
    "len(accepted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39de7a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/phuongle/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "#Remove Punctuation\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    " \n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "punctuation = list(punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fe356072",
   "metadata": {},
   "outputs": [],
   "source": [
    "functionWords = [\n",
    "    \"I\", \"a\",\n",
    "    \"an\",\n",
    "    \"the\",\n",
    "    \"and\",\n",
    "    \"but\",\n",
    "    \"or\",\n",
    "    \"as\",\n",
    "    \"if\",\n",
    "    \"when\",\n",
    "    \"than\",\n",
    "    \"because\",\n",
    "    \"though\",\n",
    "    \"although\",\n",
    "    \"while\",\n",
    "    \"where\",\n",
    "    \"after\",\n",
    "    \"before\",\n",
    "    \"since\",\n",
    "    \"until\",\n",
    "    \"by\",\n",
    "    \"with\",\n",
    "    \"without\",\n",
    "    \"under\",\n",
    "    \"over\",\n",
    "    \"in\",\n",
    "    \"on\",\n",
    "    \"at\",\n",
    "    \"to\",\n",
    "    \"from\",\n",
    "    \"into\",\n",
    "    \"onto\",\n",
    "    \"out\",\n",
    "    \"off\",\n",
    "    \"up\",\n",
    "    \"down\",\n",
    "    \"through\",\n",
    "    \"around\",\n",
    "    \"about\",\n",
    "    \"above\",\n",
    "    \"below\",\n",
    "    \"near\",\n",
    "    \"far\",\n",
    "    \"along\",\n",
    "    \"across\",\n",
    "    \"behind\",\n",
    "    \"beside\",\n",
    "    \"between\",\n",
    "    \"beyond\",\n",
    "    \"inside\",\n",
    "    \"outside\",\n",
    "    \"throughout\",\n",
    "    \"toward\",\n",
    "    \"towards\",\n",
    "    \"via\",\n",
    "    \"among\",\n",
    "    \"amongst\",\n",
    "    \"within\",\n",
    "    \"without\",\n",
    "    \"ago\",\n",
    "    \"now\",\n",
    "    \"just\",\n",
    "    \"already\",\n",
    "    \"still\",\n",
    "    \"even\",\n",
    "    \"only\",\n",
    "    \"almost\",\n",
    "    \"nearly\",\n",
    "    \"perhaps\",\n",
    "    \"maybe\",\n",
    "    \"certainly\",\n",
    "    \"surely\",\n",
    "    \"really\",\n",
    "    \"truly\",\n",
    "    \"sincerely\",\n",
    "    \"actually\",\n",
    "    \"definitely\",\n",
    "    \"practically\",\n",
    "    \"ultimately\",\n",
    "    \"basically\",\n",
    "    \"generally\",\n",
    "    \"mostly\",\n",
    "    \"often\",\n",
    "    \"sometimes\",\n",
    "    \"rarely\",\n",
    "    \"seldom\",\n",
    "    \"never\",\n",
    "    \"ever\",\n",
    "    \"always\",\n",
    "    \"together\",\n",
    "    \"apart\",\n",
    "    \"thus\",\n",
    "    \"therefore\",\n",
    "    \"hence\",\n",
    "    \"so\",\n",
    "    \"then\",\n",
    "    \"nowadays\",\n",
    "    \"meanwhile\",\n",
    "    \"forthwith\",\n",
    "    \"later\",\n",
    "    \"sooner\",\n",
    "    \"instead\",\n",
    "    \"nevertheless\",\n",
    "    \"however\",\n",
    "    \"furthermore\",\n",
    "    \"moreover\",\n",
    "    \"in addition\",\n",
    "    \"in contrast\",\n",
    "    \"in fact\",\n",
    "    \"indeed\",\n",
    "    \"that\",\n",
    "    \"what\",\n",
    "    \"which\",\n",
    "    \"who\",\n",
    "    \"whom\",\n",
    "    \"whose\",\n",
    "    \"where\",\n",
    "    \"when\",\n",
    "    \"why\",\n",
    "    \"how\",\n",
    "    \"thats\",\n",
    "    '\"',\n",
    "    \" \",\n",
    "    '’'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5dd96e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_tokens = word_tokenize(dataset[0])\n",
    "len(tokens)\n",
    "tokens = list(set(duplicated_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6c38256c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actually',\n",
       " 'I',\n",
       " 'seen',\n",
       " 'the',\n",
       " 'bell',\n",
       " 'got',\n",
       " 'o',\n",
       " 'can',\n",
       " 'rang',\n",
       " 'stork',\n",
       " 'saw',\n",
       " 've',\n",
       " 'joke😂😂😂',\n",
       " 'animals',\n",
       " 'phewww😂',\n",
       " 'now',\n",
       " '’',\n",
       " 'baby',\n",
       " 'after',\n",
       " 'and',\n",
       " 'thought',\n",
       " 'until',\n",
       " 'swear',\n",
       " 'do',\n",
       " 'cause',\n",
       " 'what']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5c999185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seen', 'bell', 'got', 'rang', 'stork', 'saw', 'joke😂😂😂', 'animals', 'phewww😂', 'baby', 'thought', 'swear', 'cause']\n"
     ]
    }
   ],
   "source": [
    "accepted_list = []\n",
    "# cleaned_tokens = [token for token in tokens if token not in stopwords or token not in functionWords and token not in punctuation]  \n",
    "cleaned_tokens = [token.lower() for token in tokens if token.lower() not in stopwords and token.lower() not in functionWords and token.lower() not in punctuation]\n",
    "accepted_list.append(cleaned_tokens)\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8f84abbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seen', 'seen bell', 'seen bell got', 'seen bell got rang', 'seen bell got rang stork', 'seen bell got rang stork saw', 'seen bell got rang stork saw joke😂😂😂', 'seen bell got rang stork saw joke😂😂😂 animals', 'seen bell got rang stork saw joke😂😂😂 animals phewww😂', 'seen bell got rang stork saw joke😂😂😂 animals phewww😂 baby', 'seen bell got rang stork saw joke😂😂😂 animals phewww😂 baby thought', 'seen bell got rang stork saw joke😂😂😂 animals phewww😂 baby thought swear', 'seen bell got rang stork saw joke😂😂😂 animals phewww😂 baby thought swear cause', 'bell', 'bell got', 'bell got rang', 'bell got rang stork', 'bell got rang stork saw', 'bell got rang stork saw joke😂😂😂', 'bell got rang stork saw joke😂😂😂 animals', 'bell got rang stork saw joke😂😂😂 animals phewww😂', 'bell got rang stork saw joke😂😂😂 animals phewww😂 baby', 'bell got rang stork saw joke😂😂😂 animals phewww😂 baby thought', 'bell got rang stork saw joke😂😂😂 animals phewww😂 baby thought swear', 'bell got rang stork saw joke😂😂😂 animals phewww😂 baby thought swear cause', 'got', 'got rang', 'got rang stork', 'got rang stork saw', 'got rang stork saw joke😂😂😂', 'got rang stork saw joke😂😂😂 animals', 'got rang stork saw joke😂😂😂 animals phewww😂', 'got rang stork saw joke😂😂😂 animals phewww😂 baby', 'got rang stork saw joke😂😂😂 animals phewww😂 baby thought', 'got rang stork saw joke😂😂😂 animals phewww😂 baby thought swear', 'got rang stork saw joke😂😂😂 animals phewww😂 baby thought swear cause', 'rang', 'rang stork', 'rang stork saw', 'rang stork saw joke😂😂😂', 'rang stork saw joke😂😂😂 animals', 'rang stork saw joke😂😂😂 animals phewww😂', 'rang stork saw joke😂😂😂 animals phewww😂 baby', 'rang stork saw joke😂😂😂 animals phewww😂 baby thought', 'rang stork saw joke😂😂😂 animals phewww😂 baby thought swear', 'rang stork saw joke😂😂😂 animals phewww😂 baby thought swear cause', 'stork', 'stork saw', 'stork saw joke😂😂😂', 'stork saw joke😂😂😂 animals', 'stork saw joke😂😂😂 animals phewww😂', 'stork saw joke😂😂😂 animals phewww😂 baby', 'stork saw joke😂😂😂 animals phewww😂 baby thought', 'stork saw joke😂😂😂 animals phewww😂 baby thought swear', 'stork saw joke😂😂😂 animals phewww😂 baby thought swear cause', 'saw', 'saw joke😂😂😂', 'saw joke😂😂😂 animals', 'saw joke😂😂😂 animals phewww😂', 'saw joke😂😂😂 animals phewww😂 baby', 'saw joke😂😂😂 animals phewww😂 baby thought', 'saw joke😂😂😂 animals phewww😂 baby thought swear', 'saw joke😂😂😂 animals phewww😂 baby thought swear cause', 'joke😂😂😂', 'joke😂😂😂 animals', 'joke😂😂😂 animals phewww😂', 'joke😂😂😂 animals phewww😂 baby', 'joke😂😂😂 animals phewww😂 baby thought', 'joke😂😂😂 animals phewww😂 baby thought swear', 'joke😂😂😂 animals phewww😂 baby thought swear cause', 'animals', 'animals phewww😂', 'animals phewww😂 baby', 'animals phewww😂 baby thought', 'animals phewww😂 baby thought swear', 'animals phewww😂 baby thought swear cause', 'phewww😂', 'phewww😂 baby', 'phewww😂 baby thought', 'phewww😂 baby thought swear', 'phewww😂 baby thought swear cause', 'baby', 'baby thought', 'baby thought swear', 'baby thought swear cause', 'thought', 'thought swear', 'thought swear cause', 'swear', 'swear cause', 'cause']\n"
     ]
    }
   ],
   "source": [
    "# generate a list of all possible combinations of words in order\n",
    "combinations = []\n",
    "for i in range(len(cleaned_tokens)):\n",
    "    for j in range(i+1, len(cleaned_tokens)+1):\n",
    "        combinations.append(\" \".join(cleaned_tokens[i:j]))\n",
    "\n",
    "print(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "84efab63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seen', 'seen bell', 'seen bell got', 'seen bell got rang', 'seen bell got rang stork', 'seen bell got rang stork saw', 'seen bell got rang stork saw joke😂😂😂', 'seen bell got rang stork saw joke😂😂😂 animals', 'seen bell got rang stork saw joke😂😂😂 animals phewww😂']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# calculate the frequency distribution of the words\n",
    "freq_dist = FreqDist(combinations)\n",
    "\n",
    "# determine the number of unique words in the text\n",
    "num_unique_words = len(freq_dist)\n",
    "\n",
    "# calculate the number of words to include in the top 25%\n",
    "num_top_words = int(num_unique_words * 0.1)\n",
    "\n",
    "# construct a list of the top 25% most common words\n",
    "top_words = [word for word, freq in freq_dist.most_common(num_top_words)]\n",
    "\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a4989f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'seen': 1, 'seen bell': 1, 'seen bell got': 1, 'seen bell got rang': 1, 'seen bell got rang stork': 1, 'seen bell got rang stork saw': 1, 'seen bell got rang stork saw joke😂😂😂': 1, 'seen bell got rang stork saw joke😂😂😂 animals': 1, 'seen bell got rang stork saw joke😂😂😂 animals phewww😂': 1, 'seen bell got rang stork saw joke😂😂😂 animals phewww😂 baby': 1, ...})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
