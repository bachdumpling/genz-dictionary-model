{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a063150c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tutorial: https://www.analyticssteps.com/blogs/nltk-python-tutorial-beginners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c1197fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bachdumpling/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/bachdumpling/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "#load library\n",
    "import requests\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fec04f69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I swear I thought the stork actually rang the bell until o saw the baby and got the joke😂😂😂 cause after what I’ve seen animals can do now phewww😂', 'am I the only one wondering how she did this?!', 'You gotta number for that stork, cause i want one.', 'Omg how cute is this!!! When she/he asks where I came from you have proof', 'so cute🥰', 'This is amazing', 'You can order them on Amazon and they send it to your door along with the baby', 'My stork was lost for many years. Thankfully they found their way to my home 💕. This is just so precious 🥺', 'I KNEW THATS HOW IT HAPPENS!!!!', 'That’s soooo cute!', 'This is so cute!!!', 'The lighting, sounds 🥺 this is pure perfection!!', 'That is one of the best things I have seen on this app!! Congratulations 🎉💕', 'So creative! love it!!! Congratulations!', 'Omg HOW COOL', '😂😂😂😂do they accept returns', 'This is so adorable. Love this💗💗', 'That is toooooo cute💜', 'congrats for a New born', '🥰🥰', 'That dog said to hell with your new carpet 😂', '“That’s a new carpet” the dog “new you say 😏“', 'the Chihuahua stress level with those kittens pulling up', 'the first one 💀', 'The 2 cats when the dog ran😂😂🤣', 'all the dogs understood all the assignments', 'the dog and the carpet omg 😂', 'bruh not the new carpeet💀💀💀', 'The last one!', 'he sed:fluff your new carpet', 'That chihuahua was scared of them kittens😂', 'long lives THE KING', '@orksococosoe THE NEW CARPET MADE ME GIGGLE', '@mikufan_39 omg', 'The \"Mario Dies\" theme at the end 🤣🤣🤣', '🥰😂', '😂😂😂😂😂', 'THE KITTENS ARE SO CUTEEEEE', '@thehybridyt 💀', '😂🥰😂🥰', 'so cute 😂😂😂', 'so sweet', 'it\\'s the middle goat\\'s \"meeee\"😅😅😅', 'Omg so cute!', 'The 2nd one cracked me up 😂😂😂😂😂😂😂😂😂😂😂', 'I need to start a tiny goat family of my own🥺', 'YOU ALL HAVE A WHOLE SYMPHONY. LOVE THE SOLO BABY AT THE END..', 'First one all I could hear is Toy Story... \"The CLAWWWWW\" 😂', 'Whew! I feel better now 😊', 'Why, of the hundreds of tiktok I saw tonight, this one make me to laugh to tears? 😂', 'Nowadays animals really clever 😂😂😂', 'Choir present best song -nothing better then singing with your bodies😂', 'Just too adorable 🥰 🥰', 'Omg so cute', 'That’s how I’m feeling too…mehhh!', \"😂😂😂make my day watching this oh my G don't kill them\", 'In America, its Baaaa! 😳', \"They're so cuteeee 😭\", \"That's it I need a farm\", 'I’m gonna watch this every day, just because it’s the best thing I’ve ever seen and it makes me smile🥰', 'I miss when my cats were kittens', 'I wish cats could stay kittens forever', '4 years and 20 pounds later, mine still has no person space', 'I miss that. the softness, the weight. the paw on my face and listening breathe. now. they do curl next to me at night. Cats are the best.', 'The love of a cat makes life worthwhile.', 'Omg I cant take the cuteness overload 😍😍U0001faf6❤️❤️', 'Yeah they’re like that when they were little mine is not doing this in a year I miss that🥺', 'he’s so real for this', 'my kitty does the same ...she has to be close to me at all times...i am her personal property', 'When my cat was a kitten she would always sleep on my neck and this one night I woke up to her sprawled all over my neck😭', 'I miss when my cat was a kitten now she doesn’t even want to sit near me 😭', 'i need a cat', 'the tail mustache 🥰', 'This is why i want a cat', 'miss when my cats were babies and would actually cuddle w me :(', 'i miss this..', 'I WANT IT', 'i miss when my cat did this man', 'This is so cute', 'That’s a baby cat', '😀', '🤣😂🤣😂', '😂😂😂😂😂', '😂😂😂', '😂😂😂', '🥰😂', '😂😂😂', '😅😂😂😂', '😂😂😂😂', '😂😂😂', '😂😂😂', '😁', '😂😂', '😂', '😂😂😂😂', '😂😂', '😂😂😂😂', '😂', '😂😂😂', 'pov you older siblings asking for a sip', \"ok ok it's all urs\", 'Polite burp', 'I can’t I’d hiccup straight away 😂😂', 'How can she do that', '\"can I have a sip?\"', \"don't really get it?\", '@ㄒ尺丨乂 me when I say can i have a sip of your drink', 'I would be crying U0001f979', '@explodingburgerzz me when im home alone', 'Me with Diet Coke', '@caseyteaxx me with cola 😂', 'Defo hungover', '@JU0001faac you', 'me in the gym on the treadmill 😂', 'fr me with Radnor fizz', '@U0001fae7 so you', 'I drank a bottle like this once and I threw up loads after', \"@heisenberg_gn and it's cold...I feel pain\", '@tessie♡ the burps are so couqette', 'I refuse to believe she uses even half of that', 'sephora dupe', 'so satisfying but so much waste😢', 'half of these products are never going to get touched again', 'it just kept going', 'i think she likes makeup guys', \"It's giving drumline :)\", 'and my mom tells me i own too much makeup💀', 'Me in 5 years', 'if only I had the money 💔', 'I don’t have that much', 'ILL TAKE a COUPLE 🐥😭', 'How is this even sustainable? Environmentally & use wise😪', 'same', 'Shawty buy every product of each brand', 'How many of those you think are expired?', 'someone tell me why i watched this whole thing', 'i aspire to be like this', 'Imagine how expensive all this is', 'That’s my favorite hole life savings lol', 'Best story ever! I need a part 2 from baby!', 'i love baby babble so much', 'baby fever, Baby Fever, BABY FEVER!!!', 'the wrist roll 😭😭😭 so cute', 'babies listening to their own voices is just so cute!', \"It's funny to think about how babies probably don't know when they say their first word because they probably think they're talking all the time.\", 'Omg her covering her mouth while yawning… ugh adorable ☺️', 'That was the most in depth story ever! I was hooked', \"that's how her day went😂😂\", 'the covering her mouth for the yawn 🥺❤️', 'i love baby voices 🥺🥺', 'oh my goodness she is so precious U0001f979', 'The caption U0001f979U0001f979', '🥱🥱🥱oh my goodness🥰🥰🥰', \"it's like she's telling a very serious story about dada!😂😂🥰🥰🥰\", 'I don’t want another baby i don’t want another baby i don’t want another baby 😂😂😩😩😩😩', 'her covering her yawn 🥺', 'OMG . So cute 🥰🥰', 'She is so adorable!!', 'Oh no she covered her mouth yawning!!!! That is so cute', 'Oh no she covered her mouth yawning!!!! That is so cute']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load json data from github \n",
    "# import urllib library\n",
    "from urllib.request import urlopen\n",
    "  \n",
    "# import json\n",
    "import json\n",
    "# store the URL in url as \n",
    "# parameter for urlopen\n",
    "url = \"https://raw.githubusercontent.com/bachdumpling/genz-dictionary-model/main/comments.json\"\n",
    "  \n",
    "# store the response of URL\n",
    "response = urlopen(url)\n",
    "  \n",
    "# storing the JSON response \n",
    "# from url in data\n",
    "dataset = json.loads(response.read())\n",
    "  \n",
    "# print the json response\n",
    "print(dataset)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "274686c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'swear', 'I', 'thought', 'the', 'stork', 'actually', 'rang', 'the', 'bell', 'until', 'o', 'saw', 'the', 'baby', 'and', 'got', 'the', 'joke😂😂😂', 'cause', 'after', 'what', 'I', '’', 've', 'seen', 'animals', 'can', 'do', 'now', 'phewww😂'], ['am', 'I', 'the', 'only', 'one', 'wondering', 'how', 'she', 'did', 'this', '?', '!'], ['You', 'got', 'ta', 'number', 'for', 'that', 'stork', ',', 'cause', 'i', 'want', 'one', '.'], ['Omg', 'how', 'cute', 'is', 'this', '!', '!', '!', 'When', 'she/he', 'asks', 'where', 'I', 'came', 'from', 'you', 'have', 'proof'], ['so', 'cute🥰'], ['This', 'is', 'amazing'], ['You', 'can', 'order', 'them', 'on', 'Amazon', 'and', 'they', 'send', 'it', 'to', 'your', 'door', 'along', 'with', 'the', 'baby'], ['My', 'stork', 'was', 'lost', 'for', 'many', 'years', '.', 'Thankfully', 'they', 'found', 'their', 'way', 'to', 'my', 'home', '💕', '.', 'This', 'is', 'just', 'so', 'precious', '🥺'], ['I', 'KNEW', 'THATS', 'HOW', 'IT', 'HAPPENS', '!', '!', '!', '!'], ['That', '’', 's', 'soooo', 'cute', '!'], ['This', 'is', 'so', 'cute', '!', '!', '!'], ['The', 'lighting', ',', 'sounds', '🥺', 'this', 'is', 'pure', 'perfection', '!', '!'], ['That', 'is', 'one', 'of', 'the', 'best', 'things', 'I', 'have', 'seen', 'on', 'this', 'app', '!', '!', 'Congratulations', '🎉💕'], ['So', 'creative', '!', 'love', 'it', '!', '!', '!', 'Congratulations', '!'], ['Omg', 'HOW', 'COOL'], ['😂😂😂😂do', 'they', 'accept', 'returns'], ['This', 'is', 'so', 'adorable', '.', 'Love', 'this💗💗'], ['That', 'is', 'toooooo', 'cute💜'], ['congrats', 'for', 'a', 'New', 'born'], ['🥰🥰'], ['That', 'dog', 'said', 'to', 'hell', 'with', 'your', 'new', 'carpet', '😂'], ['“', 'That', '’', 's', 'a', 'new', 'carpet', '”', 'the', 'dog', '“', 'new', 'you', 'say', '😏', '“'], ['the', 'Chihuahua', 'stress', 'level', 'with', 'those', 'kittens', 'pulling', 'up'], ['the', 'first', 'one', '💀'], ['The', '2', 'cats', 'when', 'the', 'dog', 'ran😂😂🤣'], ['all', 'the', 'dogs', 'understood', 'all', 'the', 'assignments'], ['the', 'dog', 'and', 'the', 'carpet', 'omg', '😂'], ['bruh', 'not', 'the', 'new', 'carpeet💀💀💀'], ['The', 'last', 'one', '!'], ['he', 'sed', ':', 'fluff', 'your', 'new', 'carpet'], ['That', 'chihuahua', 'was', 'scared', 'of', 'them', 'kittens😂'], ['long', 'lives', 'THE', 'KING'], ['@', 'orksococosoe', 'THE', 'NEW', 'CARPET', 'MADE', 'ME', 'GIGGLE'], ['@', 'mikufan_39', 'omg'], ['The', '``', 'Mario', 'Dies', \"''\", 'theme', 'at', 'the', 'end', '🤣🤣🤣'], ['🥰😂'], ['😂😂😂😂😂'], ['THE', 'KITTENS', 'ARE', 'SO', 'CUTEEEEE'], ['@', 'thehybridyt', '💀'], ['😂🥰😂🥰'], ['so', 'cute', '😂😂😂'], ['so', 'sweet'], ['it', \"'s\", 'the', 'middle', 'goat', \"'s\", '``', 'meeee', \"''\", '😅😅😅'], ['Omg', 'so', 'cute', '!'], ['The', '2nd', 'one', 'cracked', 'me', 'up', '😂😂😂😂😂😂😂😂😂😂😂'], ['I', 'need', 'to', 'start', 'a', 'tiny', 'goat', 'family', 'of', 'my', 'own🥺'], ['YOU', 'ALL', 'HAVE', 'A', 'WHOLE', 'SYMPHONY', '.', 'LOVE', 'THE', 'SOLO', 'BABY', 'AT', 'THE', 'END', '..'], ['First', 'one', 'all', 'I', 'could', 'hear', 'is', 'Toy', 'Story', '...', '``', 'The', 'CLAWWWWW', \"''\", '😂'], ['Whew', '!', 'I', 'feel', 'better', 'now', '😊'], ['Why', ',', 'of', 'the', 'hundreds', 'of', 'tiktok', 'I', 'saw', 'tonight', ',', 'this', 'one', 'make', 'me', 'to', 'laugh', 'to', 'tears', '?', '😂'], ['Nowadays', 'animals', 'really', 'clever', '😂😂😂'], ['Choir', 'present', 'best', 'song', '-nothing', 'better', 'then', 'singing', 'with', 'your', 'bodies😂'], ['Just', 'too', 'adorable', '🥰', '🥰'], ['Omg', 'so', 'cute'], ['That', '’', 's', 'how', 'I', '’', 'm', 'feeling', 'too…mehhh', '!'], ['😂😂😂make', 'my', 'day', 'watching', 'this', 'oh', 'my', 'G', 'do', \"n't\", 'kill', 'them'], ['In', 'America', ',', 'its', 'Baaaa', '!', '😳'], ['They', \"'re\", 'so', 'cuteeee', '😭'], ['That', \"'s\", 'it', 'I', 'need', 'a', 'farm'], ['I', '’', 'm', 'gon', 'na', 'watch', 'this', 'every', 'day', ',', 'just', 'because', 'it', '’', 's', 'the', 'best', 'thing', 'I', '’', 've', 'ever', 'seen', 'and', 'it', 'makes', 'me', 'smile🥰'], ['I', 'miss', 'when', 'my', 'cats', 'were', 'kittens'], ['I', 'wish', 'cats', 'could', 'stay', 'kittens', 'forever'], ['4', 'years', 'and', '20', 'pounds', 'later', ',', 'mine', 'still', 'has', 'no', 'person', 'space'], ['I', 'miss', 'that', '.', 'the', 'softness', ',', 'the', 'weight', '.', 'the', 'paw', 'on', 'my', 'face', 'and', 'listening', 'breathe', '.', 'now', '.', 'they', 'do', 'curl', 'next', 'to', 'me', 'at', 'night', '.', 'Cats', 'are', 'the', 'best', '.'], ['The', 'love', 'of', 'a', 'cat', 'makes', 'life', 'worthwhile', '.'], ['Omg', 'I', 'cant', 'take', 'the', 'cuteness', 'overload', '😍😍U0001faf6❤️❤️'], ['Yeah', 'they', '’', 're', 'like', 'that', 'when', 'they', 'were', 'little', 'mine', 'is', 'not', 'doing', 'this', 'in', 'a', 'year', 'I', 'miss', 'that🥺'], ['he', '’', 's', 'so', 'real', 'for', 'this'], ['my', 'kitty', 'does', 'the', 'same', '...', 'she', 'has', 'to', 'be', 'close', 'to', 'me', 'at', 'all', 'times', '...', 'i', 'am', 'her', 'personal', 'property'], ['When', 'my', 'cat', 'was', 'a', 'kitten', 'she', 'would', 'always', 'sleep', 'on', 'my', 'neck', 'and', 'this', 'one', 'night', 'I', 'woke', 'up', 'to', 'her', 'sprawled', 'all', 'over', 'my', 'neck😭'], ['I', 'miss', 'when', 'my', 'cat', 'was', 'a', 'kitten', 'now', 'she', 'doesn', '’', 't', 'even', 'want', 'to', 'sit', 'near', 'me', '😭'], ['i', 'need', 'a', 'cat'], ['the', 'tail', 'mustache', '🥰'], ['This', 'is', 'why', 'i', 'want', 'a', 'cat'], ['miss', 'when', 'my', 'cats', 'were', 'babies', 'and', 'would', 'actually', 'cuddle', 'w', 'me', ':', '('], ['i', 'miss', 'this', '..'], ['I', 'WANT', 'IT'], ['i', 'miss', 'when', 'my', 'cat', 'did', 'this', 'man'], ['This', 'is', 'so', 'cute'], ['That', '’', 's', 'a', 'baby', 'cat'], ['😀'], ['🤣😂🤣😂'], ['😂😂😂😂😂'], ['😂😂😂'], ['😂😂😂'], ['🥰😂'], ['😂😂😂'], ['😅😂😂😂'], ['😂😂😂😂'], ['😂😂😂'], ['😂😂😂'], ['😁'], ['😂😂'], ['😂'], ['😂😂😂😂'], ['😂😂'], ['😂😂😂😂'], ['😂'], ['😂😂😂'], ['pov', 'you', 'older', 'siblings', 'asking', 'for', 'a', 'sip'], ['ok', 'ok', 'it', \"'s\", 'all', 'urs'], ['Polite', 'burp'], ['I', 'can', '’', 't', 'I', '’', 'd', 'hiccup', 'straight', 'away', '😂😂'], ['How', 'can', 'she', 'do', 'that'], ['``', 'can', 'I', 'have', 'a', 'sip', '?', \"''\"], ['do', \"n't\", 'really', 'get', 'it', '?'], ['@', 'ㄒ尺丨乂', 'me', 'when', 'I', 'say', 'can', 'i', 'have', 'a', 'sip', 'of', 'your', 'drink'], ['I', 'would', 'be', 'crying', 'U0001f979'], ['@', 'explodingburgerzz', 'me', 'when', 'im', 'home', 'alone'], ['Me', 'with', 'Diet', 'Coke'], ['@', 'caseyteaxx', 'me', 'with', 'cola', '😂'], ['Defo', 'hungover'], ['@', 'JU0001faac', 'you'], ['me', 'in', 'the', 'gym', 'on', 'the', 'treadmill', '😂'], ['fr', 'me', 'with', 'Radnor', 'fizz'], ['@', 'U0001fae7', 'so', 'you'], ['I', 'drank', 'a', 'bottle', 'like', 'this', 'once', 'and', 'I', 'threw', 'up', 'loads', 'after'], ['@', 'heisenberg_gn', 'and', 'it', \"'s\", 'cold', '...', 'I', 'feel', 'pain'], ['@', 'tessie♡', 'the', 'burps', 'are', 'so', 'couqette'], ['I', 'refuse', 'to', 'believe', 'she', 'uses', 'even', 'half', 'of', 'that'], ['sephora', 'dupe'], ['so', 'satisfying', 'but', 'so', 'much', 'waste😢'], ['half', 'of', 'these', 'products', 'are', 'never', 'going', 'to', 'get', 'touched', 'again'], ['it', 'just', 'kept', 'going'], ['i', 'think', 'she', 'likes', 'makeup', 'guys'], ['It', \"'s\", 'giving', 'drumline', ':', ')'], ['and', 'my', 'mom', 'tells', 'me', 'i', 'own', 'too', 'much', 'makeup💀'], ['Me', 'in', '5', 'years'], ['if', 'only', 'I', 'had', 'the', 'money', '💔'], ['I', 'don', '’', 't', 'have', 'that', 'much'], ['ILL', 'TAKE', 'a', 'COUPLE', '🐥😭'], ['How', 'is', 'this', 'even', 'sustainable', '?', 'Environmentally', '&', 'use', 'wise😪'], ['same'], ['Shawty', 'buy', 'every', 'product', 'of', 'each', 'brand'], ['How', 'many', 'of', 'those', 'you', 'think', 'are', 'expired', '?'], ['someone', 'tell', 'me', 'why', 'i', 'watched', 'this', 'whole', 'thing'], ['i', 'aspire', 'to', 'be', 'like', 'this'], ['Imagine', 'how', 'expensive', 'all', 'this', 'is'], ['That', '’', 's', 'my', 'favorite', 'hole', 'life', 'savings', 'lol'], ['Best', 'story', 'ever', '!', 'I', 'need', 'a', 'part', '2', 'from', 'baby', '!'], ['i', 'love', 'baby', 'babble', 'so', 'much'], ['baby', 'fever', ',', 'Baby', 'Fever', ',', 'BABY', 'FEVER', '!', '!', '!'], ['the', 'wrist', 'roll', '😭😭😭', 'so', 'cute'], ['babies', 'listening', 'to', 'their', 'own', 'voices', 'is', 'just', 'so', 'cute', '!'], ['It', \"'s\", 'funny', 'to', 'think', 'about', 'how', 'babies', 'probably', 'do', \"n't\", 'know', 'when', 'they', 'say', 'their', 'first', 'word', 'because', 'they', 'probably', 'think', 'they', \"'re\", 'talking', 'all', 'the', 'time', '.'], ['Omg', 'her', 'covering', 'her', 'mouth', 'while', 'yawning…', 'ugh', 'adorable', '☺️'], ['That', 'was', 'the', 'most', 'in', 'depth', 'story', 'ever', '!', 'I', 'was', 'hooked'], ['that', \"'s\", 'how', 'her', 'day', 'went😂😂'], ['the', 'covering', 'her', 'mouth', 'for', 'the', 'yawn', '🥺❤️'], ['i', 'love', 'baby', 'voices', '🥺🥺'], ['oh', 'my', 'goodness', 'she', 'is', 'so', 'precious', 'U0001f979'], ['The', 'caption', 'U0001f979U0001f979'], ['🥱🥱🥱oh', 'my', 'goodness🥰🥰🥰'], ['it', \"'s\", 'like', 'she', \"'s\", 'telling', 'a', 'very', 'serious', 'story', 'about', 'dada', '!', '😂😂🥰🥰🥰'], ['I', 'don', '’', 't', 'want', 'another', 'baby', 'i', 'don', '’', 't', 'want', 'another', 'baby', 'i', 'don', '’', 't', 'want', 'another', 'baby', '😂😂😩😩😩😩'], ['her', 'covering', 'her', 'yawn', '🥺'], ['OMG', '.', 'So', 'cute', '🥰🥰'], ['She', 'is', 'so', 'adorable', '!', '!'], ['Oh', 'no', 'she', 'covered', 'her', 'mouth', 'yawning', '!', '!', '!', '!', 'That', 'is', 'so', 'cute'], ['Oh', 'no', 'she', 'covered', 'her', 'mouth', 'yawning', '!', '!', '!', '!', 'That', 'is', 'so', 'cute']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Cleaning \n",
    "accepted_sentence = []\n",
    "\n",
    "#Word Tokenization\n",
    "for sentence in dataset: \n",
    "    tokenized = word_tokenize(sentence)\n",
    "    accepted_sentence.append(tokenized)\n",
    "\n",
    "# 'This is amazing' = [\"this\", \"is\", \"amazing\"]\n",
    "\n",
    "print(accepted_sentence)\n",
    "len(accepted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39de7a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bachdumpling/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove Punctuation\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    " \n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "punctuation = list(punctuation)\n",
    "\n",
    "# print(punctuation)\n",
    "# print(stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe356072",
   "metadata": {},
   "outputs": [],
   "source": [
    "functionWords = [\n",
    "    \"I\", \"a\",\n",
    "    \"an\",\n",
    "    \"the\",\n",
    "    \"and\",\n",
    "    \"but\",\n",
    "    \"or\",\n",
    "    \"as\",\n",
    "    \"if\",\n",
    "    \"when\",\n",
    "    \"than\",\n",
    "    \"because\",\n",
    "    \"though\",\n",
    "    \"although\",\n",
    "    \"while\",\n",
    "    \"where\",\n",
    "    \"after\",\n",
    "    \"before\",\n",
    "    \"since\",\n",
    "    \"until\",\n",
    "    \"by\",\n",
    "    \"with\",\n",
    "    \"without\",\n",
    "    \"under\",\n",
    "    \"over\",\n",
    "    \"in\",\n",
    "    \"on\",\n",
    "    \"at\",\n",
    "    \"to\",\n",
    "    \"from\",\n",
    "    \"into\",\n",
    "    \"onto\",\n",
    "    \"out\",\n",
    "    \"off\",\n",
    "    \"up\",\n",
    "    \"down\",\n",
    "    \"through\",\n",
    "    \"around\",\n",
    "    \"about\",\n",
    "    \"above\",\n",
    "    \"below\",\n",
    "    \"near\",\n",
    "    \"far\",\n",
    "    \"along\",\n",
    "    \"across\",\n",
    "    \"behind\",\n",
    "    \"beside\",\n",
    "    \"between\",\n",
    "    \"beyond\",\n",
    "    \"inside\",\n",
    "    \"outside\",\n",
    "    \"throughout\",\n",
    "    \"toward\",\n",
    "    \"towards\",\n",
    "    \"via\",\n",
    "    \"among\",\n",
    "    \"amongst\",\n",
    "    \"within\",\n",
    "    \"without\",\n",
    "    \"ago\",\n",
    "    \"now\",\n",
    "    \"just\",\n",
    "    \"already\",\n",
    "    \"still\",\n",
    "    \"even\",\n",
    "    \"only\",\n",
    "    \"almost\",\n",
    "    \"nearly\",\n",
    "    \"perhaps\",\n",
    "    \"maybe\",\n",
    "    \"certainly\",\n",
    "    \"surely\",\n",
    "    \"really\",\n",
    "    \"truly\",\n",
    "    \"sincerely\",\n",
    "    \"actually\",\n",
    "    \"definitely\",\n",
    "    \"practically\",\n",
    "    \"ultimately\",\n",
    "    \"basically\",\n",
    "    \"generally\",\n",
    "    \"mostly\",\n",
    "    \"often\",\n",
    "    \"sometimes\",\n",
    "    \"rarely\",\n",
    "    \"seldom\",\n",
    "    \"never\",\n",
    "    \"ever\",\n",
    "    \"always\",\n",
    "    \"together\",\n",
    "    \"apart\",\n",
    "    \"thus\",\n",
    "    \"therefore\",\n",
    "    \"hence\",\n",
    "    \"so\",\n",
    "    \"then\",\n",
    "    \"nowadays\",\n",
    "    \"meanwhile\",\n",
    "    \"forthwith\",\n",
    "    \"later\",\n",
    "    \"sooner\",\n",
    "    \"instead\",\n",
    "    \"nevertheless\",\n",
    "    \"however\",\n",
    "    \"furthermore\",\n",
    "    \"moreover\",\n",
    "    \"in addition\",\n",
    "    \"in contrast\",\n",
    "    \"in fact\",\n",
    "    \"indeed\",\n",
    "    \"that\",\n",
    "    \"what\",\n",
    "    \"which\",\n",
    "    \"who\",\n",
    "    \"whom\",\n",
    "    \"whose\",\n",
    "    \"where\",\n",
    "    \"when\",\n",
    "    \"why\",\n",
    "    \"how\",\n",
    "    \"thats\",\n",
    "    '\"',\n",
    "    \" \",\n",
    "    '’'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dd96e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'swear', 'I', 'thought', 'the', 'stork', 'actually', 'rang', 'the', 'bell', 'until', 'o', 'saw', 'the', 'baby', 'and', 'got', 'the', 'joke😂😂😂', 'cause', 'after', 'what', 'I', '’', 've', 'seen', 'animals', 'can', 'do', 'now', 'phewww😂']\n",
      "['until', '’', 'actually', 'do', 'baby', 'got', 'after', 'thought', 'animals', 'I', 'what', 'saw', 'o', 'and', 'swear', 'stork', 'rang', 'joke😂😂😂', 'can', 've', 'cause', 'the', 'phewww😂', 'now', 'seen', 'bell']\n"
     ]
    }
   ],
   "source": [
    "duplicated_tokens = word_tokenize(dataset[0])\n",
    "print(duplicated_tokens)\n",
    "tokens = list(set(duplicated_tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c38256c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['until',\n",
       " '’',\n",
       " 'actually',\n",
       " 'do',\n",
       " 'baby',\n",
       " 'got',\n",
       " 'after',\n",
       " 'thought',\n",
       " 'animals',\n",
       " 'I',\n",
       " 'what',\n",
       " 'saw',\n",
       " 'o',\n",
       " 'and',\n",
       " 'swear',\n",
       " 'stork',\n",
       " 'rang',\n",
       " 'joke😂😂😂',\n",
       " 'can',\n",
       " 've',\n",
       " 'cause',\n",
       " 'the',\n",
       " 'phewww😂',\n",
       " 'now',\n",
       " 'seen',\n",
       " 'bell']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c999185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['baby', 'got', 'thought', 'animals', 'saw', 'swear', 'stork', 'rang', 'joke😂😂😂', 'cause', 'phewww😂', 'seen', 'bell']\n"
     ]
    }
   ],
   "source": [
    "accepted_list = []\n",
    "# cleaned_tokens = [token for token in tokens if token not in stopwords or token not in functionWords and token not in punctuation]  \n",
    "cleaned_tokens = [token.lower() for token in tokens if token.lower() not in stopwords and token.lower() not in functionWords and token.lower() not in punctuation]\n",
    "accepted_list.append(cleaned_tokens)\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f84abbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['baby', 'baby got', 'baby got thought', 'baby got thought animals', 'baby got thought animals saw', 'baby got thought animals saw swear', 'baby got thought animals saw swear stork', 'baby got thought animals saw swear stork rang', 'baby got thought animals saw swear stork rang joke😂😂😂', 'baby got thought animals saw swear stork rang joke😂😂😂 cause', 'baby got thought animals saw swear stork rang joke😂😂😂 cause phewww😂', 'baby got thought animals saw swear stork rang joke😂😂😂 cause phewww😂 seen', 'baby got thought animals saw swear stork rang joke😂😂😂 cause phewww😂 seen bell', 'got', 'got thought', 'got thought animals', 'got thought animals saw', 'got thought animals saw swear', 'got thought animals saw swear stork', 'got thought animals saw swear stork rang', 'got thought animals saw swear stork rang joke😂😂😂', 'got thought animals saw swear stork rang joke😂😂😂 cause', 'got thought animals saw swear stork rang joke😂😂😂 cause phewww😂', 'got thought animals saw swear stork rang joke😂😂😂 cause phewww😂 seen', 'got thought animals saw swear stork rang joke😂😂😂 cause phewww😂 seen bell', 'thought', 'thought animals', 'thought animals saw', 'thought animals saw swear', 'thought animals saw swear stork', 'thought animals saw swear stork rang', 'thought animals saw swear stork rang joke😂😂😂', 'thought animals saw swear stork rang joke😂😂😂 cause', 'thought animals saw swear stork rang joke😂😂😂 cause phewww😂', 'thought animals saw swear stork rang joke😂😂😂 cause phewww😂 seen', 'thought animals saw swear stork rang joke😂😂😂 cause phewww😂 seen bell', 'animals', 'animals saw', 'animals saw swear', 'animals saw swear stork', 'animals saw swear stork rang', 'animals saw swear stork rang joke😂😂😂', 'animals saw swear stork rang joke😂😂😂 cause', 'animals saw swear stork rang joke😂😂😂 cause phewww😂', 'animals saw swear stork rang joke😂😂😂 cause phewww😂 seen', 'animals saw swear stork rang joke😂😂😂 cause phewww😂 seen bell', 'saw', 'saw swear', 'saw swear stork', 'saw swear stork rang', 'saw swear stork rang joke😂😂😂', 'saw swear stork rang joke😂😂😂 cause', 'saw swear stork rang joke😂😂😂 cause phewww😂', 'saw swear stork rang joke😂😂😂 cause phewww😂 seen', 'saw swear stork rang joke😂😂😂 cause phewww😂 seen bell', 'swear', 'swear stork', 'swear stork rang', 'swear stork rang joke😂😂😂', 'swear stork rang joke😂😂😂 cause', 'swear stork rang joke😂😂😂 cause phewww😂', 'swear stork rang joke😂😂😂 cause phewww😂 seen', 'swear stork rang joke😂😂😂 cause phewww😂 seen bell', 'stork', 'stork rang', 'stork rang joke😂😂😂', 'stork rang joke😂😂😂 cause', 'stork rang joke😂😂😂 cause phewww😂', 'stork rang joke😂😂😂 cause phewww😂 seen', 'stork rang joke😂😂😂 cause phewww😂 seen bell', 'rang', 'rang joke😂😂😂', 'rang joke😂😂😂 cause', 'rang joke😂😂😂 cause phewww😂', 'rang joke😂😂😂 cause phewww😂 seen', 'rang joke😂😂😂 cause phewww😂 seen bell', 'joke😂😂😂', 'joke😂😂😂 cause', 'joke😂😂😂 cause phewww😂', 'joke😂😂😂 cause phewww😂 seen', 'joke😂😂😂 cause phewww😂 seen bell', 'cause', 'cause phewww😂', 'cause phewww😂 seen', 'cause phewww😂 seen bell', 'phewww😂', 'phewww😂 seen', 'phewww😂 seen bell', 'seen', 'seen bell', 'bell']\n"
     ]
    }
   ],
   "source": [
    "# generate a list of all possible combinations of words in order\n",
    "combinations = []\n",
    "for i in range(len(cleaned_tokens)):\n",
    "    for j in range(i+1, len(cleaned_tokens)+1):\n",
    "        combinations.append(\" \".join(cleaned_tokens[i:j]))\n",
    "\n",
    "print(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d459ac6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 91 samples and 91 outcomes>\n",
      "91\n",
      "9\n",
      "['baby', 'baby got', 'baby got thought', 'baby got thought animals', 'baby got thought animals saw', 'baby got thought animals saw swear', 'baby got thought animals saw swear stork', 'baby got thought animals saw swear stork rang', 'baby got thought animals saw swear stork rang joke😂😂😂']\n",
      "<FreqDist with 9 samples and 9 outcomes>\n"
     ]
    }
   ],
   "source": [
    "freq_dist = FreqDist(combinations)\n",
    "print(freq_dist)\n",
    "\n",
    "num_unique_words = len(freq_dist)\n",
    "print(num_unique_words)\n",
    "\n",
    "num_top_words = int(num_unique_words * 0.1)\n",
    "print(num_top_words)\n",
    "\n",
    "top_words = [word for word, freq in freq_dist.most_common(num_top_words)]\n",
    "print(top_words)\n",
    "\n",
    "result = FreqDist(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84efab63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['baby', 'baby got', 'baby got thought', 'baby got thought animals', 'baby got thought animals saw', 'baby got thought animals saw swear', 'baby got thought animals saw swear stork', 'baby got thought animals saw swear stork rang', 'baby got thought animals saw swear stork rang joke😂😂😂']\n"
     ]
    }
   ],
   "source": [
    "# calculate the frequency distribution of the words\n",
    "freq_dist = FreqDist(combinations)\n",
    "\n",
    "# determine the number of unique words in the text\n",
    "num_unique_words = len(freq_dist)\n",
    "\n",
    "# calculate the number of words to include in the top x%\n",
    "num_top_words = int(num_unique_words * 0.1)\n",
    "\n",
    "# construct a list of the top 25% most common words\n",
    "top_words = [word for word, freq in freq_dist.most_common(num_top_words)]\n",
    "\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4989f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'baby': 1, 'baby got': 1, 'baby got thought': 1, 'baby got thought animals': 1, 'baby got thought animals saw': 1, 'baby got thought animals saw swear': 1, 'baby got thought animals saw swear stork': 1, 'baby got thought animals saw swear stork rang': 1, 'baby got thought animals saw swear stork rang joke😂😂😂': 1, 'baby got thought animals saw swear stork rang joke😂😂😂 cause': 1, ...})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b15f42a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'baby': 1, 'baby got': 1, 'baby got thought': 1, 'baby got thought animals': 1, 'baby got thought animals saw': 1, 'baby got thought animals saw swear': 1, 'baby got thought animals saw swear stork': 1, 'baby got thought animals saw swear stork rang': 1, 'baby got thought animals saw swear stork rang joke😂😂😂': 1})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
