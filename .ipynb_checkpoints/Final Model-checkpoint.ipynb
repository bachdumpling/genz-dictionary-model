{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "979d8a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bachdumpling/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/bachdumpling/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#load library\n",
    "import requests\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import itertools\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96d07cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I swear I thought the stork actually rang the bell until o saw the baby and got the joke😂😂😂 cause after what I’ve seen animals can do now phewww😂', 'am I the only one wondering how she did this?!', 'You gotta number for that stork, cause i want one.', 'Omg how cute is this!!! When she/he asks where I came from you have proof', 'so cute🥰', 'This is amazing', 'You can order them on Amazon and they send it to your door along with the baby', 'My stork was lost for many years. Thankfully they found their way to my home 💕. This is just so precious 🥺', 'I KNEW THATS HOW IT HAPPENS!!!!', 'That’s soooo cute!', 'This is so cute!!!', 'The lighting, sounds 🥺 this is pure perfection!!', 'That is one of the best things I have seen on this app!! Congratulations 🎉💕', 'So creative! love it!!! Congratulations!', 'Omg HOW COOL', '😂😂😂😂do they accept returns', 'This is so adorable. Love this💗💗', 'That is toooooo cute💜', 'congrats for a New born', '🥰🥰', 'That dog said to hell with your new carpet 😂', '“That’s a new carpet” the dog “new you say 😏“', 'the Chihuahua stress level with those kittens pulling up', 'the first one 💀', 'The 2 cats when the dog ran😂😂🤣', 'all the dogs understood all the assignments', 'the dog and the carpet omg 😂', 'bruh not the new carpeet💀💀💀', 'The last one!', 'he sed:fluff your new carpet', 'That chihuahua was scared of them kittens😂', 'long lives THE KING', '@orksococosoe THE NEW CARPET MADE ME GIGGLE', '@mikufan_39 omg', 'The \"Mario Dies\" theme at the end 🤣🤣🤣', '🥰😂', '😂😂😂😂😂', 'THE KITTENS ARE SO CUTEEEEE', '@thehybridyt 💀', '😂🥰😂🥰', 'so cute 😂😂😂', 'so sweet', 'it\\'s the middle goat\\'s \"meeee\"😅😅😅', 'Omg so cute!', 'The 2nd one cracked me up 😂😂😂😂😂😂😂😂😂😂😂', 'I need to start a tiny goat family of my own🥺', 'YOU ALL HAVE A WHOLE SYMPHONY. LOVE THE SOLO BABY AT THE END..', 'First one all I could hear is Toy Story... \"The CLAWWWWW\" 😂', 'Whew! I feel better now 😊', 'Why, of the hundreds of tiktok I saw tonight, this one make me to laugh to tears? 😂', 'Nowadays animals really clever 😂😂😂', 'Choir present best song -nothing better then singing with your bodies😂', 'Just too adorable 🥰 🥰', 'Omg so cute', 'That’s how I’m feeling too…mehhh!', \"😂😂😂make my day watching this oh my G don't kill them\", 'In America, its Baaaa! 😳', \"They're so cuteeee 😭\", \"That's it I need a farm\", 'I’m gonna watch this every day, just because it’s the best thing I’ve ever seen and it makes me smile🥰', 'I miss when my cats were kittens', 'I wish cats could stay kittens forever', '4 years and 20 pounds later, mine still has no person space', 'I miss that. the softness, the weight. the paw on my face and listening breathe. now. they do curl next to me at night. Cats are the best.', 'The love of a cat makes life worthwhile.', 'Omg I cant take the cuteness overload 😍😍U0001faf6❤️❤️', 'Yeah they’re like that when they were little mine is not doing this in a year I miss that🥺', 'he’s so real for this', 'my kitty does the same ...she has to be close to me at all times...i am her personal property', 'When my cat was a kitten she would always sleep on my neck and this one night I woke up to her sprawled all over my neck😭', 'I miss when my cat was a kitten now she doesn’t even want to sit near me 😭', 'i need a cat', 'the tail mustache 🥰', 'This is why i want a cat', 'miss when my cats were babies and would actually cuddle w me :(', 'i miss this..', 'I WANT IT', 'i miss when my cat did this man', 'This is so cute', 'That’s a baby cat', '😀', '🤣😂🤣😂', '😂😂😂😂😂', '😂😂😂', '😂😂😂', '🥰😂', '😂😂😂', '😅😂😂😂', '😂😂😂😂', '😂😂😂', '😂😂😂', '😁', '😂😂', '😂', '😂😂😂😂', '😂😂', '😂😂😂😂', '😂', '😂😂😂', 'pov you older siblings asking for a sip', \"ok ok it's all urs\", 'Polite burp', 'I can’t I’d hiccup straight away 😂😂', 'How can she do that', '\"can I have a sip?\"', \"don't really get it?\", '@ㄒ尺丨乂 me when I say can i have a sip of your drink', 'I would be crying U0001f979', '@explodingburgerzz me when im home alone', 'Me with Diet Coke', '@caseyteaxx me with cola 😂', 'Defo hungover', '@JU0001faac you', 'me in the gym on the treadmill 😂', 'fr me with Radnor fizz', '@U0001fae7 so you', 'I drank a bottle like this once and I threw up loads after', \"@heisenberg_gn and it's cold...I feel pain\", '@tessie♡ the burps are so couqette', 'I refuse to believe she uses even half of that', 'sephora dupe', 'so satisfying but so much waste😢', 'half of these products are never going to get touched again', 'it just kept going', 'i think she likes makeup guys', \"It's giving drumline :)\", 'and my mom tells me i own too much makeup💀', 'Me in 5 years', 'if only I had the money 💔', 'I don’t have that much', 'ILL TAKE a COUPLE 🐥😭', 'How is this even sustainable? Environmentally & use wise😪', 'same', 'Shawty buy every product of each brand', 'How many of those you think are expired?', 'someone tell me why i watched this whole thing', 'i aspire to be like this', 'Imagine how expensive all this is', 'That’s my favorite hole life savings lol', 'Best story ever! I need a part 2 from baby!', 'i love baby babble so much', 'baby fever, Baby Fever, BABY FEVER!!!', 'the wrist roll 😭😭😭 so cute', 'babies listening to their own voices is just so cute!', \"It's funny to think about how babies probably don't know when they say their first word because they probably think they're talking all the time.\", 'Omg her covering her mouth while yawning… ugh adorable ☺️', 'That was the most in depth story ever! I was hooked', \"that's how her day went😂😂\", 'the covering her mouth for the yawn 🥺❤️', 'i love baby voices 🥺🥺', 'oh my goodness she is so precious U0001f979', 'The caption U0001f979U0001f979', '🥱🥱🥱oh my goodness🥰🥰🥰', \"it's like she's telling a very serious story about dada!😂😂🥰🥰🥰\", 'I don’t want another baby i don’t want another baby i don’t want another baby 😂😂😩😩😩😩', 'her covering her yawn 🥺', 'OMG . So cute 🥰🥰', 'She is so adorable!!', 'Oh no she covered her mouth yawning!!!! That is so cute', 'Oh no she covered her mouth yawning!!!! That is so cute']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load json data from github \n",
    "# import urllib library\n",
    "from urllib.request import urlopen\n",
    "  \n",
    "# import json\n",
    "import json\n",
    "# store the URL in url as \n",
    "# parameter for urlopen\n",
    "url = \"https://raw.githubusercontent.com/bachdumpling/genz-dictionary-model/main/comments.json\"\n",
    "  \n",
    "# store the response of URL\n",
    "response = urlopen(url)\n",
    "  \n",
    "# storing the JSON response \n",
    "# from url in data\n",
    "dataset = json.loads(response.read())\n",
    "  \n",
    "# print the json response\n",
    "print(dataset)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceaef79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bachdumpling/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove Punctuation\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "punctuation = list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db744021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13e88954",
   "metadata": {},
   "outputs": [],
   "source": [
    "functionWords = [\n",
    "    \"I\", \"a\",\n",
    "    \"an\",\n",
    "    \"the\",\n",
    "    \"and\",\n",
    "    \"but\",\n",
    "    \"or\",\n",
    "    \"as\",\n",
    "    \"if\",\n",
    "    \"when\",\n",
    "    \"than\",\n",
    "    \"because\",\n",
    "    \"though\",\n",
    "    \"although\",\n",
    "    \"while\",\n",
    "    \"where\",\n",
    "    \"after\",\n",
    "    \"before\",\n",
    "    \"since\",\n",
    "    \"until\",\n",
    "    \"by\",\n",
    "    \"with\",\n",
    "    \"without\",\n",
    "    \"under\",\n",
    "    \"over\",\n",
    "    \"in\",\n",
    "    \"on\",\n",
    "    \"at\",\n",
    "    \"to\",\n",
    "    \"from\",\n",
    "    \"into\",\n",
    "    \"onto\",\n",
    "    \"out\",\n",
    "    \"off\",\n",
    "    \"up\",\n",
    "    \"down\",\n",
    "    \"through\",\n",
    "    \"around\",\n",
    "    \"about\",\n",
    "    \"above\",\n",
    "    \"below\",\n",
    "    \"near\",\n",
    "    \"far\",\n",
    "    \"along\",\n",
    "    \"across\",\n",
    "    \"behind\",\n",
    "    \"beside\",\n",
    "    \"between\",\n",
    "    \"beyond\",\n",
    "    \"inside\",\n",
    "    \"outside\",\n",
    "    \"throughout\",\n",
    "    \"toward\",\n",
    "    \"towards\",\n",
    "    \"via\",\n",
    "    \"among\",\n",
    "    \"amongst\",\n",
    "    \"within\",\n",
    "    \"without\",\n",
    "    \"ago\",\n",
    "    \"now\",\n",
    "    \"just\",\n",
    "    \"already\",\n",
    "    \"still\",\n",
    "    \"even\",\n",
    "    \"only\",\n",
    "    \"almost\",\n",
    "    \"nearly\",\n",
    "    \"perhaps\",\n",
    "    \"maybe\",\n",
    "    \"certainly\",\n",
    "    \"surely\",\n",
    "    \"really\",\n",
    "    \"truly\",\n",
    "    \"sincerely\",\n",
    "    \"actually\",\n",
    "    \"definitely\",\n",
    "    \"practically\",\n",
    "    \"ultimately\",\n",
    "    \"basically\",\n",
    "    \"generally\",\n",
    "    \"mostly\",\n",
    "    \"often\",\n",
    "    \"sometimes\",\n",
    "    \"rarely\",\n",
    "    \"seldom\",\n",
    "    \"never\",\n",
    "    \"ever\",\n",
    "    \"always\",\n",
    "    \"together\",\n",
    "    \"apart\",\n",
    "    \"thus\",\n",
    "    \"therefore\",\n",
    "    \"hence\",\n",
    "    \"so\",\n",
    "    \"then\",\n",
    "    \"nowadays\",\n",
    "    \"meanwhile\",\n",
    "    \"forthwith\",\n",
    "    \"later\",\n",
    "    \"sooner\",\n",
    "    \"instead\",\n",
    "    \"nevertheless\",\n",
    "    \"however\",\n",
    "    \"furthermore\",\n",
    "    \"moreover\",\n",
    "    \"in addition\",\n",
    "    \"in contrast\",\n",
    "    \"in fact\",\n",
    "    \"indeed\",\n",
    "    \"that\",\n",
    "    \"what\",\n",
    "    \"which\",\n",
    "    \"who\",\n",
    "    \"whom\",\n",
    "    \"whose\",\n",
    "    \"where\",\n",
    "    \"when\",\n",
    "    \"why\",\n",
    "    \"how\",\n",
    "    \"thats\",\n",
    "    '\"',\n",
    "    \" \",\n",
    "    '’'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76a146f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "def combo(sentence):\n",
    "    duplicated_tokens = word_tokenize(sentence)\n",
    "    tokens = list(set(duplicated_tokens))\n",
    "    accepted_list = []\n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "#   cleaned_tokens = [token for token in tokens if token not in stopwords or token not in functionWords and token not in punctuation]  \n",
    "    cleaned_tokens = [token.lower() for token in tokens if token.lower() not in stopwords and token.lower() not in functionWords and token.lower() not in punctuation]\n",
    "\n",
    "    accepted_list.append(cleaned_tokens)\n",
    "    \n",
    "    combinations = []\n",
    "    for i in range(len(cleaned_tokens)):\n",
    "        for j in range(i+1, len(cleaned_tokens)+1):\n",
    "            combinations.append(\" \".join(cleaned_tokens[i:j]))\n",
    "    # calculate the frequency distribution of the words\n",
    "    freq_dist = FreqDist(combinations)\n",
    "\n",
    "    # determine the number of unique words in the text\n",
    "    num_unique_words = len(freq_dist)\n",
    "\n",
    "    # calculate the number of words to include in the top 25%\n",
    "    num_top_words = int(num_unique_words * 0.25)\n",
    "\n",
    "    # construct a list of the top 25% most common words\n",
    "    top_words = [word for word, freq in freq_dist.most_common(num_top_words)]\n",
    "    result.append(top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "579f10ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cause', 'cause phewww😂', 'cause phewww😂 seen', 'cause phewww😂 seen animals', 'cause phewww😂 seen animals thought', 'cause phewww😂 seen animals thought baby', 'cause phewww😂 seen animals thought baby bell', 'cause phewww😂 seen animals thought baby bell rang', 'cause phewww😂 seen animals thought baby bell rang swear', 'cause phewww😂 seen animals thought baby bell rang swear stork', 'cause phewww😂 seen animals thought baby bell rang swear stork saw', 'cause phewww😂 seen animals thought baby bell rang swear stork saw got', 'cause phewww😂 seen animals thought baby bell rang swear stork saw got joke😂😂😂', 'phewww😂', 'phewww😂 seen', 'phewww😂 seen animals', 'phewww😂 seen animals thought', 'phewww😂 seen animals thought baby', 'phewww😂 seen animals thought baby bell', 'phewww😂 seen animals thought baby bell rang', 'phewww😂 seen animals thought baby bell rang swear', 'phewww😂 seen animals thought baby bell rang swear stork'], [], ['cause', 'cause stork', 'cause stork want', 'cause stork want ta', 'cause stork want ta one', 'cause stork want ta one number', 'cause stork want ta one number got'], ['came', 'came cute', 'came cute asks', 'came cute asks she/he', 'came cute asks she/he proof'], [], [], ['door', 'door order', 'door order baby'], ['home', 'home 🥺', 'home 🥺 💕', 'home 🥺 💕 way', 'home 🥺 💕 way precious', 'home 🥺 💕 way precious found', 'home 🥺 💕 way precious found lost', 'home 🥺 💕 way precious found lost many', 'home 🥺 💕 way precious found lost many thankfully', 'home 🥺 💕 way precious found lost many thankfully years', 'home 🥺 💕 way precious found lost many thankfully years stork', '🥺', '🥺 💕', '🥺 💕 way', '🥺 💕 way precious', '🥺 💕 way precious found'], [], [], [], ['pure', 'pure lighting', 'pure lighting sounds'], ['app', 'app best', 'app best seen', 'app best seen 🎉💕', 'app best seen 🎉💕 one', 'app best seen 🎉💕 one things', 'app best seen 🎉💕 one things congratulations'], ['love'], [], ['returns'], ['love'], [], ['new'], [], ['said', 'said new', 'said new 😂', 'said new 😂 dog', 'said new 😂 dog carpet'], ['”', '” dog', '” dog say', '” dog say “', '” dog say “ new', '” dog say “ new 😏', '” dog say “ new 😏 carpet'], ['stress', 'stress kittens', 'stress kittens chihuahua'], ['one'], ['2', '2 dog'], ['dogs'], ['dog', 'dog 😂'], ['carpeet💀💀💀'], [], ['sed', 'sed fluff'], ['kittens😂'], ['long'], ['made', 'made giggle', 'made giggle orksococosoe'], [], ['``', '`` 🤣🤣🤣', '`` 🤣🤣🤣 theme', '`` 🤣🤣🤣 theme mario', \"`` 🤣🤣🤣 theme mario ''\", \"`` 🤣🤣🤣 theme mario '' end\", \"`` 🤣🤣🤣 theme mario '' end dies\"], [], [], [], [], [], [], [], [\"'s\", \"'s 😅😅😅\", \"'s 😅😅😅 ``\", \"'s 😅😅😅 `` meeee\", \"'s 😅😅😅 `` meeee ''\", \"'s 😅😅😅 `` meeee '' goat\", \"'s 😅😅😅 `` meeee '' goat middle\"], [], ['😂😂😂😂😂😂😂😂😂😂😂', '😂😂😂😂😂😂😂😂😂😂😂 2nd'], ['tiny', 'tiny own🥺', 'tiny own🥺 start', 'tiny own🥺 start goat', 'tiny own🥺 start goat family'], ['..', '.. whole', '.. whole baby', '.. whole baby symphony', '.. whole baby symphony love', '.. whole baby symphony love solo', '.. whole baby symphony love solo end'], ['toy', 'toy story', 'toy story ``', 'toy story `` hear', 'toy story `` hear could', \"toy story `` hear could ''\", \"toy story `` hear could '' one\", \"toy story `` hear could '' one ...\", \"toy story `` hear could '' one ... 😂\", \"toy story `` hear could '' one ... 😂 first\", \"toy story `` hear could '' one ... 😂 first clawwwww\", 'story', 'story ``', 'story `` hear', 'story `` hear could', \"story `` hear could ''\"], ['whew', 'whew feel'], ['tiktok', 'tiktok saw', 'tiktok saw tears', 'tiktok saw tears tonight', 'tiktok saw tears tonight one', 'tiktok saw tears tonight one hundreds', 'tiktok saw tears tonight one hundreds 😂', 'tiktok saw tears tonight one hundreds 😂 make', 'tiktok saw tears tonight one hundreds 😂 make laugh', 'saw', 'saw tears'], ['animals'], ['best', 'best singing', 'best singing choir', 'best singing choir bodies😂', 'best singing choir bodies😂 present', 'best singing choir bodies😂 present better', 'best singing choir bodies😂 present better song', 'best singing choir bodies😂 present better song -nothing', 'singing'], [], [], [], [\"n't\", \"n't kill\", \"n't kill watching\", \"n't kill watching oh\", \"n't kill watching oh 😂😂😂make\", \"n't kill watching oh 😂😂😂make g\", \"n't kill watching oh 😂😂😂make g day\"], ['😳'], ['cuteeee'], [\"'s\"], ['every', 'every best', 'every best day', 'every best day makes', 'every best day makes watch', 'every best day makes watch seen', 'every best day makes watch seen thing', 'every best day makes watch seen thing gon', 'every best day makes watch seen thing gon na', 'every best day makes watch seen thing gon na smile🥰', 'best', 'best day', 'best day makes'], ['kittens'], ['forever', 'forever kittens', 'forever kittens could', 'forever kittens could wish', 'forever kittens could wish stay'], ['years', 'years pounds', 'years pounds 20', 'years pounds 20 space', 'years pounds 20 space 4', 'years pounds 20 space 4 mine', 'years pounds 20 space 4 mine person'], ['softness', 'softness best', 'softness best breathe', 'softness best breathe miss', 'softness best breathe miss curl', 'softness best breathe miss curl cats', 'softness best breathe miss curl cats next', 'softness best breathe miss curl cats next paw', 'softness best breathe miss curl cats next paw face', 'softness best breathe miss curl cats next paw face weight', 'softness best breathe miss curl cats next paw face weight night', 'softness best breathe miss curl cats next paw face weight night listening', 'best', 'best breathe', 'best breathe miss', 'best breathe miss curl', 'best breathe miss curl cats', 'best breathe miss curl cats next', 'best breathe miss curl cats next paw'], ['life', 'life love', 'life love makes'], ['cant', 'cant overload', 'cant overload 😍😍u0001faf6❤️❤️', 'cant overload 😍😍u0001faf6❤️❤️ take', 'cant overload 😍😍u0001faf6❤️❤️ take omg'], ['miss', 'miss year', 'miss year like', 'miss year like that🥺', 'miss year like that🥺 little', 'miss year like that🥺 little mine', 'miss year like that🥺 little mine yeah'], [], ['personal', 'personal kitty', 'personal kitty times', 'personal kitty times close', 'personal kitty times close property'], ['kitten', 'kitten neck', 'kitten neck neck😭', 'kitten neck neck😭 one', 'kitten neck neck😭 one cat', 'kitten neck neck😭 one cat woke', 'kitten neck neck😭 one cat woke sprawled', 'kitten neck neck😭 one cat woke sprawled sleep', 'kitten neck neck😭 one cat woke sprawled sleep would', 'kitten neck neck😭 one cat woke sprawled sleep would night', 'neck', 'neck neck😭', 'neck neck😭 one'], ['miss', 'miss kitten', 'miss kitten cat', 'miss kitten cat want', 'miss kitten cat want sit'], [], ['🥰'], [], ['w', 'w babies', 'w babies miss', 'w babies miss cuddle', 'w babies miss cuddle would'], [], [], ['miss'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['sip', 'sip siblings', 'sip siblings older'], [\"'s\"], [], ['away', 'away straight'], [], ['``'], [], ['say', 'say drink'], ['u0001f979'], ['explodingburgerzz', 'explodingburgerzz alone'], [], ['caseyteaxx'], [], [], ['treadmill'], ['fizz'], [], ['threw', 'threw bottle', 'threw bottle loads'], [\"'s\", \"'s cold\", \"'s cold heisenberg_gn\", \"'s cold heisenberg_gn feel\", \"'s cold heisenberg_gn feel ...\"], ['couqette'], ['uses', 'uses believe'], [], ['waste😢'], ['going', 'going products', 'going products get'], [], ['likes', 'likes think'], [\"'s\"], ['tells', 'tells much'], [], [], [], ['take', 'take ill'], ['environmentally', 'environmentally use'], [], ['shawty', 'shawty every', 'shawty every buy'], ['many'], ['thing', 'thing someone', 'thing someone whole'], [], [], ['hole', 'hole life', 'hole life savings'], ['baby', 'baby story', 'baby story 2', 'baby story 2 part', 'baby story 2 part best'], ['babble', 'babble love'], ['fever', 'baby', 'fever fever'], ['cute', 'cute roll'], ['babies', 'babies cute'], ['know', 'know time', 'know time think', 'know time think talking', 'know time think talking word', \"know time think talking word n't\", \"know time think talking word n't funny\", \"know time think talking word n't funny 're\", \"know time think talking word n't funny 're first\", \"know time think talking word n't funny 're first 's\", \"know time think talking word n't funny 're first 's babies\", \"know time think talking word n't funny 're first 's babies probably\", \"know time think talking word n't funny 're first 's babies probably say\", 'time', 'time think', 'time think talking', 'time think talking word', \"time think talking word n't\", \"time think talking word n't funny\", \"time think talking word n't funny 're\", \"time think talking word n't funny 're first\", \"time think talking word n't funny 're first 's\"], ['covering', 'covering yawning…', 'covering yawning… ☺️', 'covering yawning… ☺️ adorable', 'covering yawning… ☺️ adorable omg', 'covering yawning… ☺️ adorable omg mouth', 'covering yawning… ☺️ adorable omg mouth ugh'], ['depth'], [\"'s\"], ['covering', 'covering yawn'], ['love', 'love baby'], ['u0001f979', 'u0001f979 oh'], [], [], [\"'s\", \"'s dada\", \"'s dada 😂😂🥰🥰🥰\", \"'s dada 😂😂🥰🥰🥰 like\", \"'s dada 😂😂🥰🥰🥰 like telling\", \"'s dada 😂😂🥰🥰🥰 like telling story\", \"'s dada 😂😂🥰🥰🥰 like telling story serious\"], ['another', 'another baby'], ['🥺'], ['cute'], [], ['oh', 'oh yawning', 'oh yawning cute'], ['oh', 'oh yawning', 'oh yawning cute']]\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    combo(item)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5a7af9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 's: 7 occurrences\n",
      "2. love: 3 occurrences\n",
      "3. best: 3 occurrences\n",
      "4. miss: 3 occurrences\n",
      "5. cause: 2 occurrences\n",
      "6. 🥺: 2 occurrences\n",
      "7. ``: 2 occurrences\n",
      "8. u0001f979: 2 occurrences\n",
      "9. baby: 2 occurrences\n",
      "10. cute: 2 occurrences\n"
     ]
    }
   ],
   "source": [
    "flat_result = []\n",
    "\n",
    "for sublist in result:\n",
    "    for item in sublist:\n",
    "        flat_result.append(item)\n",
    "\n",
    "freq_dist = FreqDist(flat_result)\n",
    "\n",
    "# determine the number of unique words in the text\n",
    "num_unique_words = len(freq_dist)\n",
    "\n",
    "# calculate the number of words to include in the top 25%\n",
    "num_top_words = int(num_unique_words * 0.1)\n",
    "\n",
    "# construct a list of the top 25% most common words\n",
    "top_words = [word for word, freq in freq_dist.most_common(num_top_words)]\n",
    "\n",
    "# print(top_words)\n",
    "\n",
    "def print_top_words(top_words):\n",
    "    for i in range(min(10, len(top_words))):\n",
    "        print(f\"{i+1}. {top_words[i]}: {freq_dist[top_words[i]]} occurrences\")\n",
    "        \n",
    "print_top_words(top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
