{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "979d8a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bachdumpling/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/bachdumpling/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#load library\n",
    "import requests\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import itertools\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96d07cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I swear I thought the stork actually rang the bell until o saw the baby and got the jokeğŸ˜‚ğŸ˜‚ğŸ˜‚ cause after what Iâ€™ve seen animals can do now phewwwğŸ˜‚', 'am I the only one wondering how she did this?!', 'You gotta number for that stork, cause i want one.', 'Omg how cute is this!!! When she/he asks where I came from you have proof', 'so cuteğŸ¥°', 'This is amazing', 'You can order them on Amazon and they send it to your door along with the baby', 'My stork was lost for many years. Thankfully they found their way to my home ğŸ’•. This is just so precious ğŸ¥º', 'I KNEW THATS HOW IT HAPPENS!!!!', 'Thatâ€™s soooo cute!', 'This is so cute!!!', 'The lighting, sounds ğŸ¥º this is pure perfection!!', 'That is one of the best things I have seen on this app!! Congratulations ğŸ‰ğŸ’•', 'So creative! love it!!! Congratulations!', 'Omg HOW COOL', 'ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚do they accept returns', 'This is so adorable. Love thisğŸ’—ğŸ’—', 'That is toooooo cuteğŸ’œ', 'congrats for a New born', 'ğŸ¥°ğŸ¥°', 'That dog said to hell with your new carpet ğŸ˜‚', 'â€œThatâ€™s a new carpetâ€ the dog â€œnew you say ğŸ˜â€œ', 'the Chihuahua stress level with those kittens pulling up', 'the first one ğŸ’€', 'The 2 cats when the dog ranğŸ˜‚ğŸ˜‚ğŸ¤£', 'all the dogs understood all the assignments', 'the dog and the carpet omg ğŸ˜‚', 'bruh not the new carpeetğŸ’€ğŸ’€ğŸ’€', 'The last one!', 'he sed:fluff your new carpet', 'That chihuahua was scared of them kittensğŸ˜‚', 'long lives THE KING', '@orksococosoe THE NEW CARPET MADE ME GIGGLE', '@mikufan_39 omg', 'The \"Mario Dies\" theme at the end ğŸ¤£ğŸ¤£ğŸ¤£', 'ğŸ¥°ğŸ˜‚', 'ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚', 'THE KITTENS ARE SO CUTEEEEE', '@thehybridyt ğŸ’€', 'ğŸ˜‚ğŸ¥°ğŸ˜‚ğŸ¥°', 'so cute ğŸ˜‚ğŸ˜‚ğŸ˜‚', 'so sweet', 'it\\'s the middle goat\\'s \"meeee\"ğŸ˜…ğŸ˜…ğŸ˜…', 'Omg so cute!', 'The 2nd one cracked me up ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚', 'I need to start a tiny goat family of my ownğŸ¥º', 'YOU ALL HAVE A WHOLE SYMPHONY. LOVE THE SOLO BABY AT THE END..', 'First one all I could hear is Toy Story... \"The CLAWWWWW\" ğŸ˜‚', 'Whew! I feel better now ğŸ˜Š', 'Why, of the hundreds of tiktok I saw tonight, this one make me to laugh to tears? ğŸ˜‚', 'Nowadays animals really clever ğŸ˜‚ğŸ˜‚ğŸ˜‚', 'Choir present best song -nothing better then singing with your bodiesğŸ˜‚', 'Just too adorable ğŸ¥° ğŸ¥°', 'Omg so cute', 'Thatâ€™s how Iâ€™m feeling tooâ€¦mehhh!', \"ğŸ˜‚ğŸ˜‚ğŸ˜‚make my day watching this oh my G don't kill them\", 'In America, its Baaaa! ğŸ˜³', \"They're so cuteeee ğŸ˜­\", \"That's it I need a farm\", 'Iâ€™m gonna watch this every day, just because itâ€™s the best thing Iâ€™ve ever seen and it makes me smileğŸ¥°', 'I miss when my cats were kittens', 'I wish cats could stay kittens forever', '4 years and 20 pounds later, mine still has no person space', 'I miss that. the softness, the weight. the paw on my face and listening breathe. now. they do curl next to me at night. Cats are the best.', 'The love of a cat makes life worthwhile.', 'Omg I cant take the cuteness overload ğŸ˜ğŸ˜U0001faf6â¤ï¸â¤ï¸', 'Yeah theyâ€™re like that when they were little mine is not doing this in a year I miss thatğŸ¥º', 'heâ€™s so real for this', 'my kitty does the same ...she has to be close to me at all times...i am her personal property', 'When my cat was a kitten she would always sleep on my neck and this one night I woke up to her sprawled all over my neckğŸ˜­', 'I miss when my cat was a kitten now she doesnâ€™t even want to sit near me ğŸ˜­', 'i need a cat', 'the tail mustache ğŸ¥°', 'This is why i want a cat', 'miss when my cats were babies and would actually cuddle w me :(', 'i miss this..', 'I WANT IT', 'i miss when my cat did this man', 'This is so cute', 'Thatâ€™s a baby cat', 'ğŸ˜€', 'ğŸ¤£ğŸ˜‚ğŸ¤£ğŸ˜‚', 'ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚', 'ğŸ˜‚ğŸ˜‚ğŸ˜‚', 'ğŸ˜‚ğŸ˜‚ğŸ˜‚', 'ğŸ¥°ğŸ˜‚', 'ğŸ˜‚ğŸ˜‚ğŸ˜‚', 'ğŸ˜…ğŸ˜‚ğŸ˜‚ğŸ˜‚', 'ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚', 'ğŸ˜‚ğŸ˜‚ğŸ˜‚', 'ğŸ˜‚ğŸ˜‚ğŸ˜‚', 'ğŸ˜', 'ğŸ˜‚ğŸ˜‚', 'ğŸ˜‚', 'ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚', 'ğŸ˜‚ğŸ˜‚', 'ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚', 'ğŸ˜‚', 'ğŸ˜‚ğŸ˜‚ğŸ˜‚', 'pov you older siblings asking for a sip', \"ok ok it's all urs\", 'Polite burp', 'I canâ€™t Iâ€™d hiccup straight away ğŸ˜‚ğŸ˜‚', 'How can she do that', '\"can I have a sip?\"', \"don't really get it?\", '@ã„’å°ºä¸¨ä¹‚ me when I say can i have a sip of your drink', 'I would be crying U0001f979', '@explodingburgerzz me when im home alone', 'Me with Diet Coke', '@caseyteaxx me with cola ğŸ˜‚', 'Defo hungover', '@JU0001faac you', 'me in the gym on the treadmill ğŸ˜‚', 'fr me with Radnor fizz', '@U0001fae7 so you', 'I drank a bottle like this once and I threw up loads after', \"@heisenberg_gn and it's cold...I feel pain\", '@tessieâ™¡ the burps are so couqette', 'I refuse to believe she uses even half of that', 'sephora dupe', 'so satisfying but so much wasteğŸ˜¢', 'half of these products are never going to get touched again', 'it just kept going', 'i think she likes makeup guys', \"It's giving drumline :)\", 'and my mom tells me i own too much makeupğŸ’€', 'Me in 5 years', 'if only I had the money ğŸ’”', 'I donâ€™t have that much', 'ILL TAKE a COUPLE ğŸ¥ğŸ˜­', 'How is this even sustainable? Environmentally & use wiseğŸ˜ª', 'same', 'Shawty buy every product of each brand', 'How many of those you think are expired?', 'someone tell me why i watched this whole thing', 'i aspire to be like this', 'Imagine how expensive all this is', 'Thatâ€™s my favorite hole life savings lol', 'Best story ever! I need a part 2 from baby!', 'i love baby babble so much', 'baby fever, Baby Fever, BABY FEVER!!!', 'the wrist roll ğŸ˜­ğŸ˜­ğŸ˜­ so cute', 'babies listening to their own voices is just so cute!', \"It's funny to think about how babies probably don't know when they say their first word because they probably think they're talking all the time.\", 'Omg her covering her mouth while yawningâ€¦ ugh adorable â˜ºï¸', 'That was the most in depth story ever! I was hooked', \"that's how her day wentğŸ˜‚ğŸ˜‚\", 'the covering her mouth for the yawn ğŸ¥ºâ¤ï¸', 'i love baby voices ğŸ¥ºğŸ¥º', 'oh my goodness she is so precious U0001f979', 'The caption U0001f979U0001f979', 'ğŸ¥±ğŸ¥±ğŸ¥±oh my goodnessğŸ¥°ğŸ¥°ğŸ¥°', \"it's like she's telling a very serious story about dada!ğŸ˜‚ğŸ˜‚ğŸ¥°ğŸ¥°ğŸ¥°\", 'I donâ€™t want another baby i donâ€™t want another baby i donâ€™t want another baby ğŸ˜‚ğŸ˜‚ğŸ˜©ğŸ˜©ğŸ˜©ğŸ˜©', 'her covering her yawn ğŸ¥º', 'OMG . So cute ğŸ¥°ğŸ¥°', 'She is so adorable!!', 'Oh no she covered her mouth yawning!!!! That is so cute', 'Oh no she covered her mouth yawning!!!! That is so cute']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load json data from github \n",
    "# import urllib library\n",
    "from urllib.request import urlopen\n",
    "  \n",
    "# import json\n",
    "import json\n",
    "# store the URL in url as \n",
    "# parameter for urlopen\n",
    "url = \"https://raw.githubusercontent.com/bachdumpling/genz-dictionary-model/main/comments.json\"\n",
    "  \n",
    "# store the response of URL\n",
    "response = urlopen(url)\n",
    "  \n",
    "# storing the JSON response \n",
    "# from url in data\n",
    "dataset = json.loads(response.read())\n",
    "  \n",
    "# print the json response\n",
    "print(dataset)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceaef79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bachdumpling/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove Punctuation\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "punctuation = list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db744021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13e88954",
   "metadata": {},
   "outputs": [],
   "source": [
    "functionWords = [\n",
    "    \"I\", \"a\",\n",
    "    \"an\",\n",
    "    \"the\",\n",
    "    \"and\",\n",
    "    \"but\",\n",
    "    \"or\",\n",
    "    \"as\",\n",
    "    \"if\",\n",
    "    \"when\",\n",
    "    \"than\",\n",
    "    \"because\",\n",
    "    \"though\",\n",
    "    \"although\",\n",
    "    \"while\",\n",
    "    \"where\",\n",
    "    \"after\",\n",
    "    \"before\",\n",
    "    \"since\",\n",
    "    \"until\",\n",
    "    \"by\",\n",
    "    \"with\",\n",
    "    \"without\",\n",
    "    \"under\",\n",
    "    \"over\",\n",
    "    \"in\",\n",
    "    \"on\",\n",
    "    \"at\",\n",
    "    \"to\",\n",
    "    \"from\",\n",
    "    \"into\",\n",
    "    \"onto\",\n",
    "    \"out\",\n",
    "    \"off\",\n",
    "    \"up\",\n",
    "    \"down\",\n",
    "    \"through\",\n",
    "    \"around\",\n",
    "    \"about\",\n",
    "    \"above\",\n",
    "    \"below\",\n",
    "    \"near\",\n",
    "    \"far\",\n",
    "    \"along\",\n",
    "    \"across\",\n",
    "    \"behind\",\n",
    "    \"beside\",\n",
    "    \"between\",\n",
    "    \"beyond\",\n",
    "    \"inside\",\n",
    "    \"outside\",\n",
    "    \"throughout\",\n",
    "    \"toward\",\n",
    "    \"towards\",\n",
    "    \"via\",\n",
    "    \"among\",\n",
    "    \"amongst\",\n",
    "    \"within\",\n",
    "    \"without\",\n",
    "    \"ago\",\n",
    "    \"now\",\n",
    "    \"just\",\n",
    "    \"already\",\n",
    "    \"still\",\n",
    "    \"even\",\n",
    "    \"only\",\n",
    "    \"almost\",\n",
    "    \"nearly\",\n",
    "    \"perhaps\",\n",
    "    \"maybe\",\n",
    "    \"certainly\",\n",
    "    \"surely\",\n",
    "    \"really\",\n",
    "    \"truly\",\n",
    "    \"sincerely\",\n",
    "    \"actually\",\n",
    "    \"definitely\",\n",
    "    \"practically\",\n",
    "    \"ultimately\",\n",
    "    \"basically\",\n",
    "    \"generally\",\n",
    "    \"mostly\",\n",
    "    \"often\",\n",
    "    \"sometimes\",\n",
    "    \"rarely\",\n",
    "    \"seldom\",\n",
    "    \"never\",\n",
    "    \"ever\",\n",
    "    \"always\",\n",
    "    \"together\",\n",
    "    \"apart\",\n",
    "    \"thus\",\n",
    "    \"therefore\",\n",
    "    \"hence\",\n",
    "    \"so\",\n",
    "    \"then\",\n",
    "    \"nowadays\",\n",
    "    \"meanwhile\",\n",
    "    \"forthwith\",\n",
    "    \"later\",\n",
    "    \"sooner\",\n",
    "    \"instead\",\n",
    "    \"nevertheless\",\n",
    "    \"however\",\n",
    "    \"furthermore\",\n",
    "    \"moreover\",\n",
    "    \"in addition\",\n",
    "    \"in contrast\",\n",
    "    \"in fact\",\n",
    "    \"indeed\",\n",
    "    \"that\",\n",
    "    \"what\",\n",
    "    \"which\",\n",
    "    \"who\",\n",
    "    \"whom\",\n",
    "    \"whose\",\n",
    "    \"where\",\n",
    "    \"when\",\n",
    "    \"why\",\n",
    "    \"how\",\n",
    "    \"thats\",\n",
    "    '\"',\n",
    "    \" \",\n",
    "    'â€™'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76a146f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "def combo(sentence):\n",
    "    duplicated_tokens = word_tokenize(sentence)\n",
    "    tokens = list(set(duplicated_tokens))\n",
    "    accepted_list = []\n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "#   cleaned_tokens = [token for token in tokens if token not in stopwords or token not in functionWords and token not in punctuation]  \n",
    "    cleaned_tokens = [token.lower() for token in tokens if token.lower() not in stopwords and token.lower() not in functionWords and token.lower() not in punctuation]\n",
    "\n",
    "    accepted_list.append(cleaned_tokens)\n",
    "    \n",
    "    combinations = []\n",
    "    for i in range(len(cleaned_tokens)):\n",
    "        for j in range(i+1, len(cleaned_tokens)+1):\n",
    "            combinations.append(\" \".join(cleaned_tokens[i:j]))\n",
    "    # calculate the frequency distribution of the words\n",
    "    freq_dist = FreqDist(combinations)\n",
    "\n",
    "    # determine the number of unique words in the text\n",
    "    num_unique_words = len(freq_dist)\n",
    "\n",
    "    # calculate the number of words to include in the top 25%\n",
    "    num_top_words = int(num_unique_words * 0.25)\n",
    "\n",
    "    # construct a list of the top 25% most common words\n",
    "    top_words = [word for word, freq in freq_dist.most_common(num_top_words)]\n",
    "    result.append(top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "579f10ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cause', 'cause phewwwğŸ˜‚', 'cause phewwwğŸ˜‚ seen', 'cause phewwwğŸ˜‚ seen animals', 'cause phewwwğŸ˜‚ seen animals thought', 'cause phewwwğŸ˜‚ seen animals thought baby', 'cause phewwwğŸ˜‚ seen animals thought baby bell', 'cause phewwwğŸ˜‚ seen animals thought baby bell rang', 'cause phewwwğŸ˜‚ seen animals thought baby bell rang swear', 'cause phewwwğŸ˜‚ seen animals thought baby bell rang swear stork', 'cause phewwwğŸ˜‚ seen animals thought baby bell rang swear stork saw', 'cause phewwwğŸ˜‚ seen animals thought baby bell rang swear stork saw got', 'cause phewwwğŸ˜‚ seen animals thought baby bell rang swear stork saw got jokeğŸ˜‚ğŸ˜‚ğŸ˜‚', 'phewwwğŸ˜‚', 'phewwwğŸ˜‚ seen', 'phewwwğŸ˜‚ seen animals', 'phewwwğŸ˜‚ seen animals thought', 'phewwwğŸ˜‚ seen animals thought baby', 'phewwwğŸ˜‚ seen animals thought baby bell', 'phewwwğŸ˜‚ seen animals thought baby bell rang', 'phewwwğŸ˜‚ seen animals thought baby bell rang swear', 'phewwwğŸ˜‚ seen animals thought baby bell rang swear stork'], [], ['cause', 'cause stork', 'cause stork want', 'cause stork want ta', 'cause stork want ta one', 'cause stork want ta one number', 'cause stork want ta one number got'], ['came', 'came cute', 'came cute asks', 'came cute asks she/he', 'came cute asks she/he proof'], [], [], ['door', 'door order', 'door order baby'], ['home', 'home ğŸ¥º', 'home ğŸ¥º ğŸ’•', 'home ğŸ¥º ğŸ’• way', 'home ğŸ¥º ğŸ’• way precious', 'home ğŸ¥º ğŸ’• way precious found', 'home ğŸ¥º ğŸ’• way precious found lost', 'home ğŸ¥º ğŸ’• way precious found lost many', 'home ğŸ¥º ğŸ’• way precious found lost many thankfully', 'home ğŸ¥º ğŸ’• way precious found lost many thankfully years', 'home ğŸ¥º ğŸ’• way precious found lost many thankfully years stork', 'ğŸ¥º', 'ğŸ¥º ğŸ’•', 'ğŸ¥º ğŸ’• way', 'ğŸ¥º ğŸ’• way precious', 'ğŸ¥º ğŸ’• way precious found'], [], [], [], ['pure', 'pure lighting', 'pure lighting sounds'], ['app', 'app best', 'app best seen', 'app best seen ğŸ‰ğŸ’•', 'app best seen ğŸ‰ğŸ’• one', 'app best seen ğŸ‰ğŸ’• one things', 'app best seen ğŸ‰ğŸ’• one things congratulations'], ['love'], [], ['returns'], ['love'], [], ['new'], [], ['said', 'said new', 'said new ğŸ˜‚', 'said new ğŸ˜‚ dog', 'said new ğŸ˜‚ dog carpet'], ['â€', 'â€ dog', 'â€ dog say', 'â€ dog say â€œ', 'â€ dog say â€œ new', 'â€ dog say â€œ new ğŸ˜', 'â€ dog say â€œ new ğŸ˜ carpet'], ['stress', 'stress kittens', 'stress kittens chihuahua'], ['one'], ['2', '2 dog'], ['dogs'], ['dog', 'dog ğŸ˜‚'], ['carpeetğŸ’€ğŸ’€ğŸ’€'], [], ['sed', 'sed fluff'], ['kittensğŸ˜‚'], ['long'], ['made', 'made giggle', 'made giggle orksococosoe'], [], ['``', '`` ğŸ¤£ğŸ¤£ğŸ¤£', '`` ğŸ¤£ğŸ¤£ğŸ¤£ theme', '`` ğŸ¤£ğŸ¤£ğŸ¤£ theme mario', \"`` ğŸ¤£ğŸ¤£ğŸ¤£ theme mario ''\", \"`` ğŸ¤£ğŸ¤£ğŸ¤£ theme mario '' end\", \"`` ğŸ¤£ğŸ¤£ğŸ¤£ theme mario '' end dies\"], [], [], [], [], [], [], [], [\"'s\", \"'s ğŸ˜…ğŸ˜…ğŸ˜…\", \"'s ğŸ˜…ğŸ˜…ğŸ˜… ``\", \"'s ğŸ˜…ğŸ˜…ğŸ˜… `` meeee\", \"'s ğŸ˜…ğŸ˜…ğŸ˜… `` meeee ''\", \"'s ğŸ˜…ğŸ˜…ğŸ˜… `` meeee '' goat\", \"'s ğŸ˜…ğŸ˜…ğŸ˜… `` meeee '' goat middle\"], [], ['ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚', 'ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ 2nd'], ['tiny', 'tiny ownğŸ¥º', 'tiny ownğŸ¥º start', 'tiny ownğŸ¥º start goat', 'tiny ownğŸ¥º start goat family'], ['..', '.. whole', '.. whole baby', '.. whole baby symphony', '.. whole baby symphony love', '.. whole baby symphony love solo', '.. whole baby symphony love solo end'], ['toy', 'toy story', 'toy story ``', 'toy story `` hear', 'toy story `` hear could', \"toy story `` hear could ''\", \"toy story `` hear could '' one\", \"toy story `` hear could '' one ...\", \"toy story `` hear could '' one ... ğŸ˜‚\", \"toy story `` hear could '' one ... ğŸ˜‚ first\", \"toy story `` hear could '' one ... ğŸ˜‚ first clawwwww\", 'story', 'story ``', 'story `` hear', 'story `` hear could', \"story `` hear could ''\"], ['whew', 'whew feel'], ['tiktok', 'tiktok saw', 'tiktok saw tears', 'tiktok saw tears tonight', 'tiktok saw tears tonight one', 'tiktok saw tears tonight one hundreds', 'tiktok saw tears tonight one hundreds ğŸ˜‚', 'tiktok saw tears tonight one hundreds ğŸ˜‚ make', 'tiktok saw tears tonight one hundreds ğŸ˜‚ make laugh', 'saw', 'saw tears'], ['animals'], ['best', 'best singing', 'best singing choir', 'best singing choir bodiesğŸ˜‚', 'best singing choir bodiesğŸ˜‚ present', 'best singing choir bodiesğŸ˜‚ present better', 'best singing choir bodiesğŸ˜‚ present better song', 'best singing choir bodiesğŸ˜‚ present better song -nothing', 'singing'], [], [], [], [\"n't\", \"n't kill\", \"n't kill watching\", \"n't kill watching oh\", \"n't kill watching oh ğŸ˜‚ğŸ˜‚ğŸ˜‚make\", \"n't kill watching oh ğŸ˜‚ğŸ˜‚ğŸ˜‚make g\", \"n't kill watching oh ğŸ˜‚ğŸ˜‚ğŸ˜‚make g day\"], ['ğŸ˜³'], ['cuteeee'], [\"'s\"], ['every', 'every best', 'every best day', 'every best day makes', 'every best day makes watch', 'every best day makes watch seen', 'every best day makes watch seen thing', 'every best day makes watch seen thing gon', 'every best day makes watch seen thing gon na', 'every best day makes watch seen thing gon na smileğŸ¥°', 'best', 'best day', 'best day makes'], ['kittens'], ['forever', 'forever kittens', 'forever kittens could', 'forever kittens could wish', 'forever kittens could wish stay'], ['years', 'years pounds', 'years pounds 20', 'years pounds 20 space', 'years pounds 20 space 4', 'years pounds 20 space 4 mine', 'years pounds 20 space 4 mine person'], ['softness', 'softness best', 'softness best breathe', 'softness best breathe miss', 'softness best breathe miss curl', 'softness best breathe miss curl cats', 'softness best breathe miss curl cats next', 'softness best breathe miss curl cats next paw', 'softness best breathe miss curl cats next paw face', 'softness best breathe miss curl cats next paw face weight', 'softness best breathe miss curl cats next paw face weight night', 'softness best breathe miss curl cats next paw face weight night listening', 'best', 'best breathe', 'best breathe miss', 'best breathe miss curl', 'best breathe miss curl cats', 'best breathe miss curl cats next', 'best breathe miss curl cats next paw'], ['life', 'life love', 'life love makes'], ['cant', 'cant overload', 'cant overload ğŸ˜ğŸ˜u0001faf6â¤ï¸â¤ï¸', 'cant overload ğŸ˜ğŸ˜u0001faf6â¤ï¸â¤ï¸ take', 'cant overload ğŸ˜ğŸ˜u0001faf6â¤ï¸â¤ï¸ take omg'], ['miss', 'miss year', 'miss year like', 'miss year like thatğŸ¥º', 'miss year like thatğŸ¥º little', 'miss year like thatğŸ¥º little mine', 'miss year like thatğŸ¥º little mine yeah'], [], ['personal', 'personal kitty', 'personal kitty times', 'personal kitty times close', 'personal kitty times close property'], ['kitten', 'kitten neck', 'kitten neck neckğŸ˜­', 'kitten neck neckğŸ˜­ one', 'kitten neck neckğŸ˜­ one cat', 'kitten neck neckğŸ˜­ one cat woke', 'kitten neck neckğŸ˜­ one cat woke sprawled', 'kitten neck neckğŸ˜­ one cat woke sprawled sleep', 'kitten neck neckğŸ˜­ one cat woke sprawled sleep would', 'kitten neck neckğŸ˜­ one cat woke sprawled sleep would night', 'neck', 'neck neckğŸ˜­', 'neck neckğŸ˜­ one'], ['miss', 'miss kitten', 'miss kitten cat', 'miss kitten cat want', 'miss kitten cat want sit'], [], ['ğŸ¥°'], [], ['w', 'w babies', 'w babies miss', 'w babies miss cuddle', 'w babies miss cuddle would'], [], [], ['miss'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['sip', 'sip siblings', 'sip siblings older'], [\"'s\"], [], ['away', 'away straight'], [], ['``'], [], ['say', 'say drink'], ['u0001f979'], ['explodingburgerzz', 'explodingburgerzz alone'], [], ['caseyteaxx'], [], [], ['treadmill'], ['fizz'], [], ['threw', 'threw bottle', 'threw bottle loads'], [\"'s\", \"'s cold\", \"'s cold heisenberg_gn\", \"'s cold heisenberg_gn feel\", \"'s cold heisenberg_gn feel ...\"], ['couqette'], ['uses', 'uses believe'], [], ['wasteğŸ˜¢'], ['going', 'going products', 'going products get'], [], ['likes', 'likes think'], [\"'s\"], ['tells', 'tells much'], [], [], [], ['take', 'take ill'], ['environmentally', 'environmentally use'], [], ['shawty', 'shawty every', 'shawty every buy'], ['many'], ['thing', 'thing someone', 'thing someone whole'], [], [], ['hole', 'hole life', 'hole life savings'], ['baby', 'baby story', 'baby story 2', 'baby story 2 part', 'baby story 2 part best'], ['babble', 'babble love'], ['fever', 'baby', 'fever fever'], ['cute', 'cute roll'], ['babies', 'babies cute'], ['know', 'know time', 'know time think', 'know time think talking', 'know time think talking word', \"know time think talking word n't\", \"know time think talking word n't funny\", \"know time think talking word n't funny 're\", \"know time think talking word n't funny 're first\", \"know time think talking word n't funny 're first 's\", \"know time think talking word n't funny 're first 's babies\", \"know time think talking word n't funny 're first 's babies probably\", \"know time think talking word n't funny 're first 's babies probably say\", 'time', 'time think', 'time think talking', 'time think talking word', \"time think talking word n't\", \"time think talking word n't funny\", \"time think talking word n't funny 're\", \"time think talking word n't funny 're first\", \"time think talking word n't funny 're first 's\"], ['covering', 'covering yawningâ€¦', 'covering yawningâ€¦ â˜ºï¸', 'covering yawningâ€¦ â˜ºï¸ adorable', 'covering yawningâ€¦ â˜ºï¸ adorable omg', 'covering yawningâ€¦ â˜ºï¸ adorable omg mouth', 'covering yawningâ€¦ â˜ºï¸ adorable omg mouth ugh'], ['depth'], [\"'s\"], ['covering', 'covering yawn'], ['love', 'love baby'], ['u0001f979', 'u0001f979 oh'], [], [], [\"'s\", \"'s dada\", \"'s dada ğŸ˜‚ğŸ˜‚ğŸ¥°ğŸ¥°ğŸ¥°\", \"'s dada ğŸ˜‚ğŸ˜‚ğŸ¥°ğŸ¥°ğŸ¥° like\", \"'s dada ğŸ˜‚ğŸ˜‚ğŸ¥°ğŸ¥°ğŸ¥° like telling\", \"'s dada ğŸ˜‚ğŸ˜‚ğŸ¥°ğŸ¥°ğŸ¥° like telling story\", \"'s dada ğŸ˜‚ğŸ˜‚ğŸ¥°ğŸ¥°ğŸ¥° like telling story serious\"], ['another', 'another baby'], ['ğŸ¥º'], ['cute'], [], ['oh', 'oh yawning', 'oh yawning cute'], ['oh', 'oh yawning', 'oh yawning cute']]\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    combo(item)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5a7af9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 's: 7 occurrences\n",
      "2. love: 3 occurrences\n",
      "3. best: 3 occurrences\n",
      "4. miss: 3 occurrences\n",
      "5. cause: 2 occurrences\n",
      "6. ğŸ¥º: 2 occurrences\n",
      "7. ``: 2 occurrences\n",
      "8. u0001f979: 2 occurrences\n",
      "9. baby: 2 occurrences\n",
      "10. cute: 2 occurrences\n"
     ]
    }
   ],
   "source": [
    "flat_result = []\n",
    "\n",
    "for sublist in result:\n",
    "    for item in sublist:\n",
    "        flat_result.append(item)\n",
    "\n",
    "freq_dist = FreqDist(flat_result)\n",
    "\n",
    "# determine the number of unique words in the text\n",
    "num_unique_words = len(freq_dist)\n",
    "\n",
    "# calculate the number of words to include in the top 25%\n",
    "num_top_words = int(num_unique_words * 0.1)\n",
    "\n",
    "# construct a list of the top 25% most common words\n",
    "top_words = [word for word, freq in freq_dist.most_common(num_top_words)]\n",
    "\n",
    "# print(top_words)\n",
    "\n",
    "def print_top_words(top_words):\n",
    "    for i in range(min(10, len(top_words))):\n",
    "        print(f\"{i+1}. {top_words[i]}: {freq_dist[top_words[i]]} occurrences\")\n",
    "        \n",
    "print_top_words(top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
